You have chosen to seed training. This will slow down your training!
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_36.txt
	(2): dataset_conaug_type = RandCat
	(3): dataset_data_aug = True
	(4): dataset_eval_case_sensitive = False
	(5): dataset_image_height = 32
	(6): dataset_image_width = 128
	(7): dataset_max_length = 25
	(8): dataset_multiscales = False
	(9): dataset_num_workers = 48
	(10): dataset_one_hot_y = True
	(11): dataset_pin_memory = True
	(12): dataset_smooth_factor = 0.1
	(13): dataset_smooth_label = False
	(14): dataset_test_batch_size = 384
	(15): dataset_test_roots = ['data/evaluation/IIIT5k_3000', 'data/evaluation/IC13_857', 'data/evaluation/CUTE80']
	(16): dataset_train_batch_size = 384
	(17): dataset_train_roots = ['data/training/ST']
	(18): dataset_use_sm = False
	(19): global_conclr = True
	(20): global_name = conclr-pretrain-vision-model
	(21): global_phase = train
	(22): global_seed = 3407
	(23): global_stage = conclr-pretrain-vision
	(24): global_workdir = workdir/conclr-pretrain-vision-model
	(25): model_checkpoint = None
	(26): model_embedding_channels = 512
	(27): model_name = modules.model_conclr_vision.ConCLR_Vision
	(28): model_strict = True
	(29): model_vision_attention = position
	(30): model_vision_backbone = transformer
	(31): model_vision_backbone_ln = 3
	(32): model_vision_loss_weight = [1.0, 0.5, 0.5]
	(33): optimizer_args_betas = (0.9, 0.999)
	(34): optimizer_bn_wd = False
	(35): optimizer_clip_grad = 20
	(36): optimizer_lr = 0.0001
	(37): optimizer_scheduler_gamma = 0.1
	(38): optimizer_scheduler_periods = [3, 1]
	(39): optimizer_true_wd = False
	(40): optimizer_type = Adam
	(41): optimizer_wd = 0.0
	(42): training_alpha = 0.2
	(43): training_epochs = 4
	(44): training_eval_iters = 1000
	(45): training_save_iters = 1000
	(46): training_show_iters = 50
	(47): training_start_iters = 0
	(48): training_stats_iters = 100000
	(49): training_tau = 2
)
Construct dataset.
6976115 training items found.
4145 valid items found.
Construct model.
ConCLR_Vision(
  (backbone): ResTranformer(
    (resnet): ResNet(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer5): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (attention): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Upsample(size=(8, 32), mode=nearest)
        (1): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
  (projection_head): ProjectionLayer(
    (fc): Linear(in_features=512, out_features=512, bias=True)
    (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): ReLU()
  )
)
Construct learner.
Use 4 GPUs.
Start training.
epoch 0 iter 50: loss = 6.4033,  smooth loss = 7.9196
epoch 0 iter 100: loss = 6.2974,  smooth loss = 6.7549
epoch 0 iter 150: loss = 6.1143,  smooth loss = 6.4361
epoch 0 iter 200: loss = 5.8953,  smooth loss = 6.1660
epoch 0 iter 250: loss = 5.8950,  smooth loss = 6.0273
epoch 0 iter 300: loss = 5.8142,  smooth loss = 5.9286
epoch 0 iter 350: loss = 5.8405,  smooth loss = 5.8506
epoch 0 iter 400: loss = 5.7036,  smooth loss = 5.8131
epoch 0 iter 450: loss = 5.7307,  smooth loss = 5.7633
epoch 0 iter 500: loss = 5.6171,  smooth loss = 5.7025
epoch 0 iter 550: loss = 5.6046,  smooth loss = 5.6663
epoch 0 iter 600: loss = 5.7308,  smooth loss = 5.6307
epoch 0 iter 650: loss = 5.5815,  smooth loss = 5.5943
epoch 0 iter 700: loss = 5.5095,  smooth loss = 5.5734
epoch 0 iter 750: loss = 5.4916,  smooth loss = 5.5384
epoch 0 iter 800: loss = 5.3791,  smooth loss = 5.4955
epoch 0 iter 850: loss = 5.4269,  smooth loss = 5.4355
epoch 0 iter 900: loss = 5.3638,  smooth loss = 5.3768
epoch 0 iter 950: loss = 5.3522,  smooth loss = 5.3278
epoch 0 iter 1000: loss = 5.2563,  smooth loss = 5.2510
average data time = 0.0365s, average running time = 0.8249s
epoch 0 iter 1000: eval loss = 3.9687,  ccr = 0.0451,  cwr = 0.0017,  ted = 17839.0000,  ned = 5810.6785,  ted/w = 4.3037, 
Better model found at epoch 0, iter 1000 with accuracy value: 0.0017.
Save model conclr-pretrain-vision-model_0_1000
You have chosen to seed training. This will slow down your training!
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_36.txt
	(2): dataset_conaug_type = RandCat
	(3): dataset_data_aug = True
	(4): dataset_eval_case_sensitive = False
	(5): dataset_image_height = 32
	(6): dataset_image_width = 128
	(7): dataset_max_length = 25
	(8): dataset_multiscales = False
	(9): dataset_num_workers = 48
	(10): dataset_one_hot_y = True
	(11): dataset_pin_memory = True
	(12): dataset_smooth_factor = 0.1
	(13): dataset_smooth_label = False
	(14): dataset_test_batch_size = 384
	(15): dataset_test_roots = ['data/evaluation/IIIT5k_3000', 'data/evaluation/IC13_857', 'data/evaluation/CUTE80']
	(16): dataset_train_batch_size = 384
	(17): dataset_train_roots = ['data/training/ST']
	(18): dataset_use_sm = False
	(19): global_conclr = True
	(20): global_name = conclr-pretrain-vision-model
	(21): global_phase = train
	(22): global_seed = 3407
	(23): global_stage = conclr-pretrain-vision
	(24): global_workdir = workdir/conclr-pretrain-vision-model
	(25): model_checkpoint = None
	(26): model_embedding_channels = 512
	(27): model_name = modules.model_conclr_vision.ConCLR_Vision
	(28): model_strict = True
	(29): model_vision_attention = position
	(30): model_vision_backbone = transformer
	(31): model_vision_backbone_ln = 3
	(32): model_vision_loss_weight = [1.0, 0.5, 0.5]
	(33): optimizer_args_betas = (0.9, 0.999)
	(34): optimizer_bn_wd = False
	(35): optimizer_clip_grad = 20
	(36): optimizer_lr = 0.0001
	(37): optimizer_scheduler_gamma = 0.1
	(38): optimizer_scheduler_periods = [3, 1]
	(39): optimizer_true_wd = False
	(40): optimizer_type = Adam
	(41): optimizer_wd = 0.0
	(42): training_alpha = 0.2
	(43): training_epochs = 4
	(44): training_eval_iters = 1000
	(45): training_save_iters = 1000
	(46): training_show_iters = 50
	(47): training_start_iters = 0
	(48): training_stats_iters = 100000
	(49): training_tau = 2
)
Construct dataset.
6976115 training items found.
4145 valid items found.
Construct model.
ConCLR_Vision(
  (backbone): ResTranformer(
    (resnet): ResNet(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer5): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (attention): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Upsample(size=(8, 32), mode=nearest)
        (1): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
  (projection_head): ProjectionLayer(
    (fc): Linear(in_features=512, out_features=512, bias=True)
    (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): ReLU()
  )
)
Construct learner.
Use 4 GPUs.
Start training.
epoch 0 iter 50: loss = 5.5840,  smooth loss = 5.6409
epoch 0 iter 100: loss = 5.3225,  smooth loss = 5.4661
epoch 0 iter 150: loss = 5.1934,  smooth loss = 5.3506
epoch 0 iter 200: loss = 5.1170,  smooth loss = 5.2503
epoch 0 iter 250: loss = 5.0496,  smooth loss = 5.1715
epoch 0 iter 300: loss = 4.9621,  smooth loss = 5.1032
epoch 0 iter 350: loss = 4.9493,  smooth loss = 5.0261
epoch 0 iter 400: loss = 4.8502,  smooth loss = 4.9712
epoch 0 iter 450: loss = 4.8684,  smooth loss = 4.8710
epoch 0 iter 500: loss = 4.5636,  smooth loss = 4.7345
epoch 0 iter 550: loss = 4.3849,  smooth loss = 4.5932
epoch 0 iter 600: loss = 4.4315,  smooth loss = 4.4361
epoch 0 iter 650: loss = 4.1339,  smooth loss = 4.2715
epoch 0 iter 700: loss = 3.9325,  smooth loss = 4.1003
epoch 0 iter 750: loss = 3.6824,  smooth loss = 3.9115
epoch 0 iter 800: loss = 3.4942,  smooth loss = 3.7296
epoch 0 iter 850: loss = 3.4568,  smooth loss = 3.5132
epoch 0 iter 900: loss = 3.0318,  smooth loss = 3.3233
epoch 0 iter 950: loss = 3.1314,  smooth loss = 3.1644
epoch 0 iter 1000: loss = 2.9159,  smooth loss = 3.0105
average data time = 0.0317s, average running time = 0.8206s
epoch 0 iter 1000: eval loss = 2.2801,  ccr = 0.3735,  cwr = 0.0842,  ted = 11482.0000,  ned = 3542.5519,  ted/w = 2.7701, 
Better model found at epoch 0, iter 1000 with accuracy value: 0.0842.
Save model conclr-pretrain-vision-model_0_1000
epoch 0 iter 1050: loss = 2.6883,  smooth loss = 2.8832
epoch 0 iter 1100: loss = 2.7732,  smooth loss = 2.7969
epoch 0 iter 1150: loss = 2.4179,  smooth loss = 2.6655
epoch 0 iter 1200: loss = 2.7066,  smooth loss = 2.6014
epoch 0 iter 1250: loss = 2.4438,  smooth loss = 2.4985
epoch 0 iter 1300: loss = 2.4247,  smooth loss = 2.4241
You have chosen to seed training. This will slow down your training!
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_36.txt
	(2): dataset_conaug_type = RandCat
	(3): dataset_data_aug = True
	(4): dataset_eval_case_sensitive = False
	(5): dataset_image_height = 32
	(6): dataset_image_width = 128
	(7): dataset_max_length = 25
	(8): dataset_multiscales = False
	(9): dataset_num_workers = 48
	(10): dataset_one_hot_y = True
	(11): dataset_pin_memory = True
	(12): dataset_smooth_factor = 0.1
	(13): dataset_smooth_label = False
	(14): dataset_test_batch_size = 384
	(15): dataset_test_roots = ['data/evaluation/IIIT5k_3000', 'data/evaluation/IC13_857', 'data/evaluation/CUTE80']
	(16): dataset_train_batch_size = 384
	(17): dataset_train_roots = ['data/training/ST']
	(18): dataset_use_sm = False
	(19): global_conclr = True
	(20): global_name = conclr-pretrain-vision-model
	(21): global_phase = train
	(22): global_seed = 3407
	(23): global_stage = conclr-pretrain-vision
	(24): global_workdir = workdir/conclr-pretrain-vision-model
	(25): model_checkpoint = None
	(26): model_embedding_channels = 512
	(27): model_name = modules.model_conclr_vision.ConCLR_Vision
	(28): model_strict = True
	(29): model_vision_attention = position
	(30): model_vision_backbone = transformer
	(31): model_vision_backbone_ln = 3
	(32): model_vision_loss_weight = [1.0, 0.5, 0.5]
	(33): optimizer_args_betas = (0.9, 0.999)
	(34): optimizer_bn_wd = False
	(35): optimizer_clip_grad = 20
	(36): optimizer_lr = 0.0001
	(37): optimizer_scheduler_gamma = 0.1
	(38): optimizer_scheduler_periods = [3, 1]
	(39): optimizer_true_wd = False
	(40): optimizer_type = Adam
	(41): optimizer_wd = 0.0
	(42): training_alpha = 0.2
	(43): training_epochs = 4
	(44): training_eval_iters = 1000
	(45): training_save_iters = 1000
	(46): training_show_iters = 50
	(47): training_start_iters = 0
	(48): training_stats_iters = 100000
	(49): training_tau = 2
)
Construct dataset.
6976115 training items found.
4145 valid items found.
Construct model.
ConCLR_Vision(
  (backbone): ResTranformer(
    (resnet): ResNet(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer5): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (attention): PositionAttention(
    (k_encoder): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (k_decoder): Sequential(
      (0): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
      (3): Sequential(
        (0): Upsample(size=(8, 32), mode=nearest)
        (1): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
      )
    )
    (pos_encoder): PositionalEncoding(
      (dropout): Dropout(p=0, inplace=False)
    )
    (project): Linear(in_features=512, out_features=512, bias=True)
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
  (projection_head): ProjectionLayer(
    (fc): Linear(in_features=512, out_features=512, bias=True)
    (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): Sigmoid()
  )
)
Construct learner.
Use 4 GPUs.
Start training.
epoch 0 iter 50: loss = 12.6022,  smooth loss = 13.1090
epoch 0 iter 100: loss = 11.8346,  smooth loss = 12.3684
epoch 0 iter 150: loss = 11.4130,  smooth loss = 11.9524
epoch 0 iter 200: loss = 11.0259,  smooth loss = 11.5930
epoch 0 iter 250: loss = 11.0740,  smooth loss = 11.3399
epoch 0 iter 300: loss = 10.9485,  smooth loss = 11.1601
epoch 0 iter 350: loss = 10.9625,  smooth loss = 11.0043
epoch 0 iter 400: loss = 10.4881,  smooth loss = 10.8701
epoch 0 iter 450: loss = 10.7067,  smooth loss = 10.7521
epoch 0 iter 500: loss = 10.1956,  smooth loss = 10.5793
epoch 0 iter 550: loss = 10.4489,  smooth loss = 10.5000
epoch 0 iter 600: loss = 10.4795,  smooth loss = 10.3986
epoch 0 iter 650: loss = 10.0740,  smooth loss = 10.3035
epoch 0 iter 700: loss = 9.9343,  smooth loss = 10.2253
epoch 0 iter 750: loss = 9.8816,  smooth loss = 10.1459
epoch 0 iter 800: loss = 9.7666,  smooth loss = 10.0578
epoch 0 iter 850: loss = 9.8322,  smooth loss = 9.9424
epoch 0 iter 900: loss = 9.6434,  smooth loss = 9.8426
epoch 0 iter 950: loss = 9.9382,  smooth loss = 9.7676
epoch 0 iter 1000: loss = 9.6400,  smooth loss = 9.6641
average data time = 0.0367s, average running time = 0.8472s
epoch 0 iter 1000: eval loss = 4.1191,  ccr = 0.0514,  cwr = 0.0027,  ted = 18019.0000,  ned = 5627.7248,  ted/w = 4.3472, 
Better model found at epoch 0, iter 1000 with accuracy value: 0.0027.
Save model conclr-pretrain-vision-model_0_1000
epoch 0 iter 1050: loss = 9.2131,  smooth loss = 9.5545
epoch 0 iter 1100: loss = 9.5572,  smooth loss = 9.4935
epoch 0 iter 1150: loss = 9.0606,  smooth loss = 9.3426
epoch 0 iter 1200: loss = 9.2498,  smooth loss = 9.2589
epoch 0 iter 1250: loss = 8.9268,  smooth loss = 9.1252
epoch 0 iter 1300: loss = 8.8341,  smooth loss = 8.9606
epoch 0 iter 1350: loss = 8.8830,  smooth loss = 8.8268
epoch 0 iter 1400: loss = 8.5499,  smooth loss = 8.6427
epoch 0 iter 1450: loss = 8.3398,  smooth loss = 8.4800
epoch 0 iter 1500: loss = 8.4490,  smooth loss = 8.3281
epoch 0 iter 1550: loss = 7.9721,  smooth loss = 8.1210
epoch 0 iter 1600: loss = 7.6266,  smooth loss = 7.9649
epoch 0 iter 1650: loss = 7.5808,  smooth loss = 7.7712
epoch 0 iter 1700: loss = 7.4893,  smooth loss = 7.5891
epoch 0 iter 1750: loss = 7.2842,  smooth loss = 7.4260
epoch 0 iter 1800: loss = 7.2144,  smooth loss = 7.2960
epoch 0 iter 1850: loss = 6.6916,  smooth loss = 7.1331
epoch 0 iter 1900: loss = 6.7644,  smooth loss = 7.0227
epoch 0 iter 1950: loss = 6.4464,  smooth loss = 6.8455
epoch 0 iter 2000: loss = 6.8525,  smooth loss = 6.7724
average data time = 0.0191s, average running time = 0.8594s
epoch 0 iter 2000: eval loss = 2.2799,  ccr = 0.4075,  cwr = 0.0854,  ted = 11051.0000,  ned = 3424.0701,  ted/w = 2.6661, 
Better model found at epoch 0, iter 2000 with accuracy value: 0.0854.
Save model conclr-pretrain-vision-model_0_2000
epoch 0 iter 2050: loss = 6.3033,  smooth loss = 6.6304
epoch 0 iter 2100: loss = 6.2649,  smooth loss = 6.4883
epoch 0 iter 2150: loss = 6.4313,  smooth loss = 6.4001
epoch 0 iter 2200: loss = 6.4341,  smooth loss = 6.3174
epoch 0 iter 2250: loss = 6.0336,  smooth loss = 6.2060
epoch 0 iter 2300: loss = 6.2406,  smooth loss = 6.1198
epoch 0 iter 2350: loss = 5.9097,  smooth loss = 6.0000
epoch 0 iter 2400: loss = 5.6983,  smooth loss = 5.9112
epoch 0 iter 2450: loss = 5.3898,  smooth loss = 5.8251
epoch 0 iter 2500: loss = 5.6716,  smooth loss = 5.7424
epoch 0 iter 2550: loss = 5.7854,  smooth loss = 5.7004
epoch 0 iter 2600: loss = 5.5375,  smooth loss = 5.6104
epoch 0 iter 2650: loss = 5.0478,  smooth loss = 5.5255
epoch 0 iter 2700: loss = 5.2809,  smooth loss = 5.4487
epoch 0 iter 2750: loss = 5.4442,  smooth loss = 5.3995
epoch 0 iter 2800: loss = 5.4656,  smooth loss = 5.3745
epoch 0 iter 2850: loss = 5.1196,  smooth loss = 5.2956
epoch 0 iter 2900: loss = 4.8683,  smooth loss = 5.2260
epoch 0 iter 2950: loss = 5.1341,  smooth loss = 5.1830
epoch 0 iter 3000: loss = 5.3659,  smooth loss = 5.1185
average data time = 0.0132s, average running time = 0.8632s
epoch 0 iter 3000: eval loss = 1.3023,  ccr = 0.6310,  cwr = 0.4268,  ted = 5866.0000,  ned = 1690.8957,  ted/w = 1.4152, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4268.
Save model conclr-pretrain-vision-model_0_3000
epoch 0 iter 3050: loss = 4.8276,  smooth loss = 5.0674
epoch 0 iter 3100: loss = 5.1723,  smooth loss = 5.0329
epoch 0 iter 3150: loss = 4.7872,  smooth loss = 4.9629
epoch 0 iter 3200: loss = 4.9766,  smooth loss = 4.9163
epoch 0 iter 3250: loss = 4.6855,  smooth loss = 4.8639
epoch 0 iter 3300: loss = 4.6982,  smooth loss = 4.8249
epoch 0 iter 3350: loss = 4.4442,  smooth loss = 4.7884
epoch 0 iter 3400: loss = 4.7710,  smooth loss = 4.7175
epoch 0 iter 3450: loss = 4.7309,  smooth loss = 4.6984
epoch 0 iter 3500: loss = 4.7310,  smooth loss = 4.6576
epoch 0 iter 3550: loss = 4.5694,  smooth loss = 4.6120
epoch 0 iter 3600: loss = 4.2418,  smooth loss = 4.5372
epoch 0 iter 3650: loss = 4.6458,  smooth loss = 4.5024
epoch 0 iter 3700: loss = 4.6537,  smooth loss = 4.5076
epoch 0 iter 3750: loss = 4.6395,  smooth loss = 4.4290
epoch 0 iter 3800: loss = 4.8224,  smooth loss = 4.4114
epoch 0 iter 3850: loss = 4.0270,  smooth loss = 4.3730
epoch 0 iter 3900: loss = 4.3855,  smooth loss = 4.3409
epoch 0 iter 3950: loss = 4.4921,  smooth loss = 4.2969
epoch 0 iter 4000: loss = 3.7930,  smooth loss = 4.2520
average data time = 0.0103s, average running time = 0.8652s
epoch 0 iter 4000: eval loss = 1.2600,  ccr = 0.6857,  cwr = 0.3587,  ted = 5731.0000,  ned = 1879.5344,  ted/w = 1.3826, 
Save model conclr-pretrain-vision-model_0_4000
epoch 0 iter 4050: loss = 4.4777,  smooth loss = 4.2486
epoch 0 iter 4100: loss = 4.0755,  smooth loss = 4.2097
epoch 0 iter 4150: loss = 4.1312,  smooth loss = 4.1409
epoch 0 iter 4200: loss = 4.1595,  smooth loss = 4.0980
epoch 0 iter 4250: loss = 3.8012,  smooth loss = 4.0745
epoch 0 iter 4300: loss = 3.8195,  smooth loss = 4.0341
epoch 0 iter 4350: loss = 4.1169,  smooth loss = 3.9844
epoch 0 iter 4400: loss = 4.0559,  smooth loss = 3.9615
epoch 0 iter 4450: loss = 4.0872,  smooth loss = 3.9655
epoch 0 iter 4500: loss = 3.8691,  smooth loss = 3.9279
epoch 0 iter 4550: loss = 3.9430,  smooth loss = 3.8741
epoch 0 iter 4600: loss = 3.9639,  smooth loss = 3.8938
epoch 0 iter 4650: loss = 3.5144,  smooth loss = 3.8453
epoch 0 iter 4700: loss = 3.8999,  smooth loss = 3.8118
epoch 0 iter 4750: loss = 3.8390,  smooth loss = 3.7981
epoch 0 iter 4800: loss = 3.6649,  smooth loss = 3.7513
epoch 0 iter 4850: loss = 3.4076,  smooth loss = 3.7106
epoch 0 iter 4900: loss = 3.8319,  smooth loss = 3.7093
epoch 0 iter 4950: loss = 3.6267,  smooth loss = 3.6739
epoch 0 iter 5000: loss = 3.7596,  smooth loss = 3.6714
average data time = 0.0086s, average running time = 0.8612s
epoch 0 iter 5000: eval loss = 0.8484,  ccr = 0.7825,  cwr = 0.5969,  ted = 3445.0000,  ned = 1067.3609,  ted/w = 0.8311, 
Better model found at epoch 0, iter 5000 with accuracy value: 0.5969.
Save model conclr-pretrain-vision-model_0_5000
epoch 0 iter 5050: loss = 3.4689,  smooth loss = 3.6171
epoch 0 iter 5100: loss = 3.5522,  smooth loss = 3.6098
epoch 0 iter 5150: loss = 3.3226,  smooth loss = 3.5723
epoch 0 iter 5200: loss = 3.0816,  smooth loss = 3.5604
epoch 0 iter 5250: loss = 3.4780,  smooth loss = 3.5117
epoch 0 iter 5300: loss = 3.3033,  smooth loss = 3.5116
epoch 0 iter 5350: loss = 3.4865,  smooth loss = 3.4814
epoch 0 iter 5400: loss = 3.6119,  smooth loss = 3.4476
epoch 0 iter 5450: loss = 3.5061,  smooth loss = 3.4304
epoch 0 iter 5500: loss = 3.4103,  smooth loss = 3.4075
epoch 0 iter 5550: loss = 3.3420,  smooth loss = 3.3975
epoch 0 iter 5600: loss = 3.3060,  smooth loss = 3.3712
epoch 0 iter 5650: loss = 3.2089,  smooth loss = 3.3415
epoch 0 iter 5700: loss = 3.3080,  smooth loss = 3.3173
epoch 0 iter 5750: loss = 3.5536,  smooth loss = 3.2998
epoch 0 iter 5800: loss = 3.1375,  smooth loss = 3.2424
epoch 0 iter 5850: loss = 3.2584,  smooth loss = 3.2790
epoch 0 iter 5900: loss = 3.1787,  smooth loss = 3.2420
epoch 0 iter 5950: loss = 3.2943,  smooth loss = 3.2019
epoch 0 iter 6000: loss = 2.9336,  smooth loss = 3.2038
average data time = 0.0074s, average running time = 0.8613s
epoch 0 iter 6000: eval loss = 0.7654,  ccr = 0.8053,  cwr = 0.6017,  ted = 3030.0000,  ned = 997.0128,  ted/w = 0.7310, 
Better model found at epoch 0, iter 6000 with accuracy value: 0.6017.
Save model conclr-pretrain-vision-model_0_6000
epoch 0 iter 6050: loss = 3.0944,  smooth loss = 3.1494
epoch 0 iter 6100: loss = 3.0451,  smooth loss = 3.1434
epoch 0 iter 6150: loss = 3.3280,  smooth loss = 3.1465
epoch 0 iter 6200: loss = 2.8311,  smooth loss = 3.0960
epoch 0 iter 6250: loss = 3.1257,  smooth loss = 3.0810
epoch 0 iter 6300: loss = 2.8366,  smooth loss = 3.0693
epoch 0 iter 6350: loss = 2.8436,  smooth loss = 3.0206
epoch 0 iter 6400: loss = 2.8752,  smooth loss = 3.0240
epoch 0 iter 6450: loss = 3.0915,  smooth loss = 3.0186
epoch 0 iter 6500: loss = 2.8953,  smooth loss = 2.9969
epoch 0 iter 6550: loss = 3.3054,  smooth loss = 2.9844
epoch 0 iter 6600: loss = 3.0836,  smooth loss = 2.9781
epoch 0 iter 6650: loss = 2.9515,  smooth loss = 2.9280
epoch 0 iter 6700: loss = 2.9579,  smooth loss = 2.9178
epoch 0 iter 6750: loss = 2.8074,  smooth loss = 2.9311
epoch 0 iter 6800: loss = 2.9169,  smooth loss = 2.9057
epoch 0 iter 6850: loss = 2.9760,  smooth loss = 2.8727
epoch 0 iter 6900: loss = 2.8274,  smooth loss = 2.8485
epoch 0 iter 6950: loss = 2.6686,  smooth loss = 2.8564
epoch 0 iter 7000: loss = 2.8504,  smooth loss = 2.8687
average data time = 0.0066s, average running time = 0.8629s
epoch 0 iter 7000: eval loss = 0.7375,  ccr = 0.8281,  cwr = 0.6591,  ted = 2840.0000,  ned = 1075.8464,  ted/w = 0.6852, 
Better model found at epoch 0, iter 7000 with accuracy value: 0.6591.
Save model conclr-pretrain-vision-model_0_7000
epoch 0 iter 7050: loss = 2.8004,  smooth loss = 2.8129
epoch 0 iter 7100: loss = 2.7379,  smooth loss = 2.7957
epoch 0 iter 7150: loss = 2.5486,  smooth loss = 2.7825
epoch 0 iter 7200: loss = 2.6274,  smooth loss = 2.7689
epoch 0 iter 7250: loss = 2.9563,  smooth loss = 2.7509
epoch 0 iter 7300: loss = 2.7059,  smooth loss = 2.7078
epoch 0 iter 7350: loss = 2.6592,  smooth loss = 2.6966
epoch 0 iter 7400: loss = 2.3887,  smooth loss = 2.7013
epoch 0 iter 7450: loss = 2.5203,  smooth loss = 2.7087
epoch 0 iter 7500: loss = 2.6074,  smooth loss = 2.6915
epoch 0 iter 7550: loss = 2.6724,  smooth loss = 2.6629
epoch 0 iter 7600: loss = 2.5667,  smooth loss = 2.6539
epoch 0 iter 7650: loss = 2.5133,  smooth loss = 2.6225
epoch 0 iter 7700: loss = 2.6346,  smooth loss = 2.6182
epoch 0 iter 7750: loss = 2.7764,  smooth loss = 2.6220
epoch 0 iter 7800: loss = 2.8769,  smooth loss = 2.6322
epoch 0 iter 7850: loss = 2.5921,  smooth loss = 2.5735
epoch 0 iter 7900: loss = 2.8551,  smooth loss = 2.5641
epoch 0 iter 7950: loss = 2.4261,  smooth loss = 2.5631
epoch 0 iter 8000: loss = 2.5176,  smooth loss = 2.5621
average data time = 0.0059s, average running time = 0.8650s
epoch 0 iter 8000: eval loss = 0.6361,  ccr = 0.8369,  cwr = 0.6649,  ted = 2488.0000,  ned = 809.2252,  ted/w = 0.6002, 
Better model found at epoch 0, iter 8000 with accuracy value: 0.6649.
Save model conclr-pretrain-vision-model_0_8000
epoch 0 iter 8050: loss = 2.5729,  smooth loss = 2.5557
epoch 0 iter 8100: loss = 2.9691,  smooth loss = 2.5492
epoch 0 iter 8150: loss = 2.3733,  smooth loss = 2.4933
epoch 0 iter 8200: loss = 2.6450,  smooth loss = 2.4739
epoch 0 iter 8250: loss = 2.6236,  smooth loss = 2.4624
epoch 0 iter 8300: loss = 2.3628,  smooth loss = 2.4541
epoch 0 iter 8350: loss = 2.2408,  smooth loss = 2.4368
epoch 0 iter 8400: loss = 2.5595,  smooth loss = 2.4484
epoch 0 iter 8450: loss = 2.3702,  smooth loss = 2.4299
epoch 0 iter 8500: loss = 2.4502,  smooth loss = 2.4293
epoch 0 iter 8550: loss = 2.4121,  smooth loss = 2.4401
epoch 0 iter 8600: loss = 2.3619,  smooth loss = 2.3999
epoch 0 iter 8650: loss = 2.3972,  smooth loss = 2.3973
epoch 0 iter 8700: loss = 2.5625,  smooth loss = 2.4027
epoch 0 iter 8750: loss = 2.5432,  smooth loss = 2.4061
epoch 0 iter 8800: loss = 2.3692,  smooth loss = 2.3563
epoch 0 iter 8850: loss = 2.5486,  smooth loss = 2.3539
epoch 0 iter 8900: loss = 2.2971,  smooth loss = 2.3589
epoch 0 iter 8950: loss = 2.0620,  smooth loss = 2.3623
epoch 0 iter 9000: loss = 2.2826,  smooth loss = 2.3222
average data time = 0.0055s, average running time = 0.8664s
epoch 0 iter 9000: eval loss = 0.6472,  ccr = 0.8416,  cwr = 0.7078,  ted = 2459.0000,  ned = 775.4665,  ted/w = 0.5932, 
Better model found at epoch 0, iter 9000 with accuracy value: 0.7078.
Save model conclr-pretrain-vision-model_0_9000
epoch 0 iter 9050: loss = 2.2854,  smooth loss = 2.2998
epoch 0 iter 9100: loss = 2.2557,  smooth loss = 2.2954
epoch 0 iter 9150: loss = 2.3523,  smooth loss = 2.3113
epoch 0 iter 9200: loss = 2.3882,  smooth loss = 2.3017
epoch 0 iter 9250: loss = 2.1892,  smooth loss = 2.2497
epoch 0 iter 9300: loss = 2.2550,  smooth loss = 2.2424
epoch 0 iter 9350: loss = 2.1254,  smooth loss = 2.2364
epoch 0 iter 9400: loss = 2.1068,  smooth loss = 2.2356
epoch 0 iter 9450: loss = 2.1793,  smooth loss = 2.2170
epoch 0 iter 9500: loss = 2.2317,  smooth loss = 2.2073
epoch 0 iter 9550: loss = 2.1521,  smooth loss = 2.1843
epoch 0 iter 9600: loss = 2.0283,  smooth loss = 2.1808
epoch 0 iter 9650: loss = 2.2830,  smooth loss = 2.1880
epoch 0 iter 9700: loss = 2.3663,  smooth loss = 2.1796
epoch 0 iter 9750: loss = 2.1198,  smooth loss = 2.1851
epoch 0 iter 9800: loss = 2.2654,  smooth loss = 2.1944
epoch 0 iter 9850: loss = 1.8947,  smooth loss = 2.1592
epoch 0 iter 9900: loss = 2.3718,  smooth loss = 2.1719
epoch 0 iter 9950: loss = 2.4012,  smooth loss = 2.1625
epoch 0 iter 10000: loss = 2.2843,  smooth loss = 2.1558
average data time = 0.0051s, average running time = 0.8670s
epoch 0 iter 10000: eval loss = 0.6548,  ccr = 0.8513,  cwr = 0.6516,  ted = 2651.0000,  ned = 990.1311,  ted/w = 0.6396, 
Save model conclr-pretrain-vision-model_0_10000
epoch 0 iter 10050: loss = 2.1279,  smooth loss = 2.1202
epoch 0 iter 10100: loss = 2.1861,  smooth loss = 2.1261
epoch 0 iter 10150: loss = 2.2441,  smooth loss = 2.1217
epoch 0 iter 10200: loss = 2.2597,  smooth loss = 2.1128
epoch 0 iter 10250: loss = 2.0552,  smooth loss = 2.0841
epoch 0 iter 10300: loss = 1.9079,  smooth loss = 2.0728
epoch 0 iter 10350: loss = 1.9803,  smooth loss = 2.0763
epoch 0 iter 10400: loss = 2.1128,  smooth loss = 2.0755
epoch 0 iter 10450: loss = 2.1650,  smooth loss = 2.0848
epoch 0 iter 10500: loss = 1.9224,  smooth loss = 2.0518
epoch 0 iter 10550: loss = 1.8612,  smooth loss = 2.0350
epoch 0 iter 10600: loss = 1.9785,  smooth loss = 2.0427
epoch 0 iter 10650: loss = 2.0266,  smooth loss = 2.0357
epoch 0 iter 10700: loss = 2.2794,  smooth loss = 2.0323
epoch 0 iter 10750: loss = 2.1292,  smooth loss = 2.0255
epoch 0 iter 10800: loss = 2.1001,  smooth loss = 2.0031
epoch 0 iter 10850: loss = 2.1889,  smooth loss = 1.9781
epoch 0 iter 10900: loss = 2.2109,  smooth loss = 2.0036
epoch 0 iter 10950: loss = 2.1403,  smooth loss = 2.0045
epoch 0 iter 11000: loss = 2.1914,  smooth loss = 1.9790
average data time = 0.0048s, average running time = 0.8662s
epoch 0 iter 11000: eval loss = 0.6331,  ccr = 0.8697,  cwr = 0.7300,  ted = 2116.0000,  ned = 722.6200,  ted/w = 0.5105, 
Better model found at epoch 0, iter 11000 with accuracy value: 0.7300.
Save model conclr-pretrain-vision-model_0_11000
epoch 0 iter 11050: loss = 1.9256,  smooth loss = 1.9446
epoch 0 iter 11100: loss = 2.0182,  smooth loss = 1.9444
epoch 0 iter 11150: loss = 1.8871,  smooth loss = 1.9706
epoch 0 iter 11200: loss = 1.8641,  smooth loss = 1.9672
epoch 0 iter 11250: loss = 2.0089,  smooth loss = 1.9484
epoch 0 iter 11300: loss = 1.7401,  smooth loss = 1.9253
epoch 0 iter 11350: loss = 1.9201,  smooth loss = 1.9309
epoch 0 iter 11400: loss = 2.0656,  smooth loss = 1.9214
epoch 0 iter 11450: loss = 1.9358,  smooth loss = 1.9287
epoch 0 iter 11500: loss = 1.9351,  smooth loss = 1.9126
epoch 0 iter 11550: loss = 1.7957,  smooth loss = 1.9020
epoch 0 iter 11600: loss = 1.8405,  smooth loss = 1.9131
epoch 0 iter 11650: loss = 2.0827,  smooth loss = 1.9114
epoch 0 iter 11700: loss = 1.7921,  smooth loss = 1.8707
epoch 0 iter 11750: loss = 1.9449,  smooth loss = 1.8697
epoch 0 iter 11800: loss = 1.8118,  smooth loss = 1.8824
epoch 0 iter 11850: loss = 1.9213,  smooth loss = 1.8869
epoch 0 iter 11900: loss = 2.0033,  smooth loss = 1.8645
epoch 0 iter 11950: loss = 1.7696,  smooth loss = 1.8782
epoch 0 iter 12000: loss = 1.8740,  smooth loss = 1.8568
average data time = 0.0045s, average running time = 0.8646s
epoch 0 iter 12000: eval loss = 0.6211,  ccr = 0.8729,  cwr = 0.7370,  ted = 2158.0000,  ned = 760.8961,  ted/w = 0.5206, 
Better model found at epoch 0, iter 12000 with accuracy value: 0.7370.
Save model conclr-pretrain-vision-model_0_12000
epoch 0 iter 12050: loss = 1.9589,  smooth loss = 1.8493
epoch 0 iter 12100: loss = 1.6581,  smooth loss = 1.8428
epoch 0 iter 12150: loss = 1.7504,  smooth loss = 1.8267
epoch 0 iter 12200: loss = 1.9830,  smooth loss = 1.8455
epoch 0 iter 12250: loss = 1.8094,  smooth loss = 1.8058
epoch 0 iter 12300: loss = 1.8934,  smooth loss = 1.8205
epoch 0 iter 12350: loss = 1.5773,  smooth loss = 1.8035
epoch 0 iter 12400: loss = 1.7132,  smooth loss = 1.7938
epoch 0 iter 12450: loss = 1.9117,  smooth loss = 1.7975
epoch 0 iter 12500: loss = 1.8596,  smooth loss = 1.7754
epoch 0 iter 12550: loss = 1.7734,  smooth loss = 1.7783
epoch 0 iter 12600: loss = 1.7607,  smooth loss = 1.7775
epoch 0 iter 12650: loss = 1.9694,  smooth loss = 1.7783
epoch 0 iter 12700: loss = 1.7773,  smooth loss = 1.7495
epoch 0 iter 12750: loss = 1.6447,  smooth loss = 1.7639
epoch 0 iter 12800: loss = 1.5810,  smooth loss = 1.7646
epoch 0 iter 12850: loss = 1.8053,  smooth loss = 1.7509
epoch 0 iter 12900: loss = 1.6992,  smooth loss = 1.7409
epoch 0 iter 12950: loss = 1.4754,  smooth loss = 1.7326
epoch 0 iter 13000: loss = 1.8415,  smooth loss = 1.7626
average data time = 0.0043s, average running time = 0.8633s
epoch 0 iter 13000: eval loss = 0.6372,  ccr = 0.8718,  cwr = 0.7103,  ted = 1948.0000,  ned = 703.4841,  ted/w = 0.4700, 
Save model conclr-pretrain-vision-model_0_13000
epoch 0 iter 13050: loss = 1.8620,  smooth loss = 1.7470
epoch 0 iter 13100: loss = 1.6201,  smooth loss = 1.7243
epoch 0 iter 13150: loss = 1.5494,  smooth loss = 1.6969
epoch 0 iter 13200: loss = 1.8913,  smooth loss = 1.6919
epoch 0 iter 13250: loss = 1.6166,  smooth loss = 1.6985
epoch 0 iter 13300: loss = 1.5814,  smooth loss = 1.6982
epoch 0 iter 13350: loss = 1.7238,  smooth loss = 1.6917
epoch 0 iter 13400: loss = 1.5348,  smooth loss = 1.6941
epoch 0 iter 13450: loss = 1.5108,  smooth loss = 1.6791
epoch 0 iter 13500: loss = 1.5871,  smooth loss = 1.6749
epoch 0 iter 13550: loss = 1.4994,  smooth loss = 1.6763
epoch 0 iter 13600: loss = 1.7038,  smooth loss = 1.6540
epoch 0 iter 13650: loss = 1.7452,  smooth loss = 1.6640
epoch 0 iter 13700: loss = 1.4444,  smooth loss = 1.6754
epoch 0 iter 13750: loss = 1.5878,  smooth loss = 1.6695
epoch 0 iter 13800: loss = 1.8063,  smooth loss = 1.6510
epoch 0 iter 13850: loss = 1.6743,  smooth loss = 1.6523
epoch 0 iter 13900: loss = 1.6717,  smooth loss = 1.6412
epoch 0 iter 13950: loss = 1.7004,  smooth loss = 1.6597
epoch 0 iter 14000: loss = 1.7772,  smooth loss = 1.6654
average data time = 0.0041s, average running time = 0.8624s
epoch 0 iter 14000: eval loss = 0.6263,  ccr = 0.8556,  cwr = 0.7233,  ted = 2108.0000,  ned = 616.7541,  ted/w = 0.5086, 
Save model conclr-pretrain-vision-model_0_14000
epoch 0 iter 14050: loss = 1.6455,  smooth loss = 1.6456
epoch 0 iter 14100: loss = 1.7989,  smooth loss = 1.6556
epoch 0 iter 14150: loss = 1.4879,  smooth loss = 1.6397
epoch 0 iter 14200: loss = 1.7895,  smooth loss = 1.6370
epoch 0 iter 14250: loss = 1.6315,  smooth loss = 1.6404
epoch 0 iter 14300: loss = 1.5294,  smooth loss = 1.6477
epoch 0 iter 14350: loss = 1.7459,  smooth loss = 1.6438
epoch 0 iter 14400: loss = 1.4701,  smooth loss = 1.6120
epoch 0 iter 14450: loss = 1.5668,  smooth loss = 1.6071
epoch 0 iter 14500: loss = 1.5171,  smooth loss = 1.6217
epoch 0 iter 14550: loss = 1.5511,  smooth loss = 1.6090
epoch 0 iter 14600: loss = 1.8764,  smooth loss = 1.6128
epoch 0 iter 14650: loss = 1.5723,  smooth loss = 1.6195
epoch 0 iter 14700: loss = 1.5669,  smooth loss = 1.6158
epoch 0 iter 14750: loss = 1.5921,  smooth loss = 1.5956
epoch 0 iter 14800: loss = 1.6709,  smooth loss = 1.5872
epoch 0 iter 14850: loss = 1.5548,  smooth loss = 1.5690
epoch 0 iter 14900: loss = 1.5291,  smooth loss = 1.5642
epoch 0 iter 14950: loss = 1.7609,  smooth loss = 1.5685
epoch 0 iter 15000: loss = 1.5923,  smooth loss = 1.5953
average data time = 0.0039s, average running time = 0.8619s
epoch 0 iter 15000: eval loss = 0.6081,  ccr = 0.8646,  cwr = 0.7264,  ted = 2125.0000,  ned = 719.8283,  ted/w = 0.5127, 
Save model conclr-pretrain-vision-model_0_15000
epoch 0 iter 15050: loss = 1.5831,  smooth loss = 1.5668
epoch 0 iter 15100: loss = 1.4298,  smooth loss = 1.5619
epoch 0 iter 15150: loss = 1.5039,  smooth loss = 1.5655
epoch 0 iter 15200: loss = 1.5336,  smooth loss = 1.5517
epoch 0 iter 15250: loss = 1.3977,  smooth loss = 1.5504
epoch 0 iter 15300: loss = 1.4528,  smooth loss = 1.5386
epoch 0 iter 15350: loss = 1.5909,  smooth loss = 1.5365
epoch 0 iter 15400: loss = 1.6489,  smooth loss = 1.5373
epoch 0 iter 15450: loss = 1.5864,  smooth loss = 1.5514
epoch 0 iter 15500: loss = 1.4696,  smooth loss = 1.5662
epoch 0 iter 15550: loss = 1.3758,  smooth loss = 1.5319
epoch 0 iter 15600: loss = 1.5492,  smooth loss = 1.5183
epoch 0 iter 15650: loss = 1.5595,  smooth loss = 1.5160
epoch 0 iter 15700: loss = 1.6391,  smooth loss = 1.5074
epoch 0 iter 15750: loss = 1.5815,  smooth loss = 1.5322
epoch 0 iter 15800: loss = 1.6255,  smooth loss = 1.5262
epoch 0 iter 15850: loss = 1.5020,  smooth loss = 1.5304
epoch 0 iter 15900: loss = 1.4062,  smooth loss = 1.4977
epoch 0 iter 15950: loss = 1.6484,  smooth loss = 1.5200
epoch 0 iter 16000: loss = 1.4506,  smooth loss = 1.5272
average data time = 0.0038s, average running time = 0.8620s
epoch 0 iter 16000: eval loss = 0.5722,  ccr = 0.8822,  cwr = 0.7375,  ted = 2044.0000,  ned = 754.8760,  ted/w = 0.4931, 
Better model found at epoch 0, iter 16000 with accuracy value: 0.7375.
Save model conclr-pretrain-vision-model_0_16000
epoch 0 iter 16050: loss = 1.5850,  smooth loss = 1.5232
epoch 0 iter 16100: loss = 1.3145,  smooth loss = 1.4941
epoch 0 iter 16150: loss = 1.4947,  smooth loss = 1.4840
epoch 0 iter 16200: loss = 1.5328,  smooth loss = 1.4982
epoch 0 iter 16250: loss = 1.6372,  smooth loss = 1.4957
epoch 0 iter 16300: loss = 1.4202,  smooth loss = 1.4737
epoch 0 iter 16350: loss = 1.4110,  smooth loss = 1.4714
epoch 0 iter 16400: loss = 1.4864,  smooth loss = 1.4904
epoch 0 iter 16450: loss = 1.6083,  smooth loss = 1.4900
epoch 0 iter 16500: loss = 1.5122,  smooth loss = 1.4700
epoch 0 iter 16550: loss = 1.4505,  smooth loss = 1.4844
epoch 0 iter 16600: loss = 1.4046,  smooth loss = 1.4599
epoch 0 iter 16650: loss = 1.5548,  smooth loss = 1.4816
epoch 0 iter 16700: loss = 1.5094,  smooth loss = 1.4680
epoch 0 iter 16750: loss = 1.3814,  smooth loss = 1.4689
epoch 0 iter 16800: loss = 1.3065,  smooth loss = 1.4688
epoch 0 iter 16850: loss = 1.6394,  smooth loss = 1.4940
epoch 0 iter 16900: loss = 1.4957,  smooth loss = 1.4652
epoch 0 iter 16950: loss = 1.3963,  smooth loss = 1.4581
epoch 0 iter 17000: loss = 1.4283,  smooth loss = 1.4499
average data time = 0.0037s, average running time = 0.8624s
epoch 0 iter 17000: eval loss = 0.6563,  ccr = 0.8642,  cwr = 0.7168,  ted = 2232.0000,  ned = 801.4935,  ted/w = 0.5385, 
Save model conclr-pretrain-vision-model_0_17000
epoch 0 iter 17050: loss = 1.5550,  smooth loss = 1.4437
epoch 0 iter 17100: loss = 1.4118,  smooth loss = 1.4573
epoch 0 iter 17150: loss = 1.4089,  smooth loss = 1.4487
epoch 0 iter 17200: loss = 1.5248,  smooth loss = 1.4667
epoch 0 iter 17250: loss = 1.3673,  smooth loss = 1.4453
epoch 0 iter 17300: loss = 1.5315,  smooth loss = 1.4412
epoch 0 iter 17350: loss = 1.2512,  smooth loss = 1.4267
epoch 0 iter 17400: loss = 1.3105,  smooth loss = 1.4169
epoch 0 iter 17450: loss = 1.6744,  smooth loss = 1.4345
epoch 0 iter 17500: loss = 1.6772,  smooth loss = 1.4328
epoch 0 iter 17550: loss = 1.3652,  smooth loss = 1.4432
epoch 0 iter 17600: loss = 1.4673,  smooth loss = 1.4366
epoch 0 iter 17650: loss = 1.4937,  smooth loss = 1.4196
epoch 0 iter 17700: loss = 1.4163,  smooth loss = 1.4299
epoch 0 iter 17750: loss = 1.6958,  smooth loss = 1.4293
epoch 0 iter 17800: loss = 1.2722,  smooth loss = 1.4190
epoch 0 iter 17850: loss = 1.3424,  smooth loss = 1.4190
epoch 0 iter 17900: loss = 1.3988,  smooth loss = 1.4138
epoch 0 iter 17950: loss = 1.3676,  smooth loss = 1.4160
epoch 0 iter 18000: loss = 1.4616,  smooth loss = 1.4222
average data time = 0.0035s, average running time = 0.8631s
epoch 0 iter 18000: eval loss = 0.6040,  ccr = 0.8730,  cwr = 0.7476,  ted = 1881.0000,  ned = 661.0398,  ted/w = 0.4538, 
Better model found at epoch 0, iter 18000 with accuracy value: 0.7476.
Save model conclr-pretrain-vision-model_0_18000
epoch 0 iter 18050: loss = 1.4154,  smooth loss = 1.4018
epoch 0 iter 18100: loss = 1.5118,  smooth loss = 1.3926
epoch 0 iter 18150: loss = 1.2645,  smooth loss = 1.3847
epoch 1 iter 18200: loss = 1.3236,  smooth loss = 1.4010
epoch 1 iter 18250: loss = 1.3319,  smooth loss = 1.4011
epoch 1 iter 18300: loss = 1.3449,  smooth loss = 1.4038
epoch 1 iter 18350: loss = 1.5770,  smooth loss = 1.3939
epoch 1 iter 18400: loss = 1.3501,  smooth loss = 1.3912
epoch 1 iter 18450: loss = 1.3677,  smooth loss = 1.3788
epoch 1 iter 18500: loss = 1.4059,  smooth loss = 1.3858
epoch 1 iter 18550: loss = 1.3678,  smooth loss = 1.3952
epoch 1 iter 18600: loss = 1.2428,  smooth loss = 1.3840
epoch 1 iter 18650: loss = 1.2694,  smooth loss = 1.3626
epoch 1 iter 18700: loss = 1.3580,  smooth loss = 1.3884
epoch 1 iter 18750: loss = 1.3899,  smooth loss = 1.3762
epoch 1 iter 18800: loss = 1.2489,  smooth loss = 1.3605
epoch 1 iter 18850: loss = 1.2431,  smooth loss = 1.3686
epoch 1 iter 18900: loss = 1.2769,  smooth loss = 1.3608
epoch 1 iter 18950: loss = 1.5005,  smooth loss = 1.3497
epoch 1 iter 19000: loss = 1.2897,  smooth loss = 1.3535
average data time = 0.0045s, average running time = 0.8637s
epoch 1 iter 19000: eval loss = 0.5471,  ccr = 0.8827,  cwr = 0.7214,  ted = 1925.0000,  ned = 734.5935,  ted/w = 0.4644, 
Save model conclr-pretrain-vision-model_1_19000
epoch 1 iter 19050: loss = 1.3162,  smooth loss = 1.3683
epoch 1 iter 19100: loss = 1.2654,  smooth loss = 1.3543
epoch 1 iter 19150: loss = 1.3045,  smooth loss = 1.3523
epoch 1 iter 19200: loss = 1.0983,  smooth loss = 1.3404
epoch 1 iter 19250: loss = 1.2956,  smooth loss = 1.3451
epoch 1 iter 19300: loss = 1.3676,  smooth loss = 1.3351
epoch 1 iter 19350: loss = 1.4163,  smooth loss = 1.3751
epoch 1 iter 19400: loss = 1.3119,  smooth loss = 1.3529
epoch 1 iter 19450: loss = 1.2195,  smooth loss = 1.3330
epoch 1 iter 19500: loss = 1.2866,  smooth loss = 1.3306
epoch 1 iter 19550: loss = 1.3452,  smooth loss = 1.3407
epoch 1 iter 19600: loss = 1.5887,  smooth loss = 1.3467
epoch 1 iter 19650: loss = 1.3831,  smooth loss = 1.3445
epoch 1 iter 19700: loss = 1.3747,  smooth loss = 1.3434
epoch 1 iter 19750: loss = 1.5412,  smooth loss = 1.3449
epoch 1 iter 19800: loss = 1.2522,  smooth loss = 1.3300
epoch 1 iter 19850: loss = 1.3820,  smooth loss = 1.3295
epoch 1 iter 19900: loss = 1.5564,  smooth loss = 1.3409
epoch 1 iter 19950: loss = 1.2519,  smooth loss = 1.3087
epoch 1 iter 20000: loss = 1.3524,  smooth loss = 1.3199
average data time = 0.0043s, average running time = 0.8640s
epoch 1 iter 20000: eval loss = 0.5817,  ccr = 0.8866,  cwr = 0.7831,  ted = 1670.0000,  ned = 579.5290,  ted/w = 0.4029, 
Better model found at epoch 1, iter 20000 with accuracy value: 0.7831.
Save model conclr-pretrain-vision-model_1_20000
epoch 1 iter 20050: loss = 1.2364,  smooth loss = 1.3229
epoch 1 iter 20100: loss = 1.3487,  smooth loss = 1.3140
epoch 1 iter 20150: loss = 1.5287,  smooth loss = 1.3233
epoch 1 iter 20200: loss = 1.2150,  smooth loss = 1.3111
epoch 1 iter 20250: loss = 1.3361,  smooth loss = 1.3120
epoch 1 iter 20300: loss = 1.2165,  smooth loss = 1.3016
epoch 1 iter 20350: loss = 1.3449,  smooth loss = 1.3211
epoch 1 iter 20400: loss = 1.2301,  smooth loss = 1.3169
epoch 1 iter 20450: loss = 1.3002,  smooth loss = 1.3118
epoch 1 iter 20500: loss = 1.3186,  smooth loss = 1.3131
epoch 1 iter 20550: loss = 1.1830,  smooth loss = 1.3125
epoch 1 iter 20600: loss = 1.3078,  smooth loss = 1.3109
epoch 1 iter 20650: loss = 1.3534,  smooth loss = 1.3123
epoch 1 iter 20700: loss = 1.2604,  smooth loss = 1.3053
epoch 1 iter 20750: loss = 1.1763,  smooth loss = 1.3024
epoch 1 iter 20800: loss = 1.2505,  smooth loss = 1.3068
epoch 1 iter 20850: loss = 1.3184,  smooth loss = 1.3075
epoch 1 iter 20900: loss = 1.2354,  smooth loss = 1.2967
epoch 1 iter 20950: loss = 1.4834,  smooth loss = 1.2995
epoch 1 iter 21000: loss = 1.1618,  smooth loss = 1.2846
average data time = 0.0042s, average running time = 0.8634s
epoch 1 iter 21000: eval loss = 0.6023,  ccr = 0.8858,  cwr = 0.7310,  ted = 1930.0000,  ned = 760.3597,  ted/w = 0.4656, 
Save model conclr-pretrain-vision-model_1_21000
epoch 1 iter 21050: loss = 1.4866,  smooth loss = 1.3043
epoch 1 iter 21100: loss = 1.2609,  smooth loss = 1.3045
epoch 1 iter 21150: loss = 1.4119,  smooth loss = 1.3064
epoch 1 iter 21200: loss = 1.5179,  smooth loss = 1.2909
epoch 1 iter 21250: loss = 1.3393,  smooth loss = 1.2831
epoch 1 iter 21300: loss = 1.1670,  smooth loss = 1.2931
epoch 1 iter 21350: loss = 1.3097,  smooth loss = 1.2811
epoch 1 iter 21400: loss = 1.2891,  smooth loss = 1.2896
epoch 1 iter 21450: loss = 1.2376,  smooth loss = 1.2962
epoch 1 iter 21500: loss = 1.3629,  smooth loss = 1.2839
epoch 1 iter 21550: loss = 1.2706,  smooth loss = 1.2876
epoch 1 iter 21600: loss = 1.3122,  smooth loss = 1.2996
epoch 1 iter 21650: loss = 1.2729,  smooth loss = 1.2743
epoch 1 iter 21700: loss = 1.3625,  smooth loss = 1.2863
epoch 1 iter 21750: loss = 1.2540,  smooth loss = 1.2661
epoch 1 iter 21800: loss = 1.2950,  smooth loss = 1.2818
epoch 1 iter 21850: loss = 1.2447,  smooth loss = 1.2725
epoch 1 iter 21900: loss = 1.2738,  smooth loss = 1.2937
epoch 1 iter 21950: loss = 1.2871,  smooth loss = 1.2759
epoch 1 iter 22000: loss = 1.2917,  smooth loss = 1.2680
average data time = 0.0041s, average running time = 0.8628s
epoch 1 iter 22000: eval loss = 0.7082,  ccr = 0.8728,  cwr = 0.7397,  ted = 1876.0000,  ned = 671.3665,  ted/w = 0.4526, 
Save model conclr-pretrain-vision-model_1_22000
epoch 1 iter 22050: loss = 1.3299,  smooth loss = 1.2671
epoch 1 iter 22100: loss = 1.3618,  smooth loss = 1.2653
epoch 1 iter 22150: loss = 1.1727,  smooth loss = 1.2735
epoch 1 iter 22200: loss = 0.9375,  smooth loss = 1.2558
epoch 1 iter 22250: loss = 1.2397,  smooth loss = 1.2551
epoch 1 iter 22300: loss = 1.3923,  smooth loss = 1.2640
epoch 1 iter 22350: loss = 1.0853,  smooth loss = 1.2551
epoch 1 iter 22400: loss = 1.3814,  smooth loss = 1.2547
epoch 1 iter 22450: loss = 1.3469,  smooth loss = 1.2528
epoch 1 iter 22500: loss = 1.2440,  smooth loss = 1.2561
epoch 1 iter 22550: loss = 1.1981,  smooth loss = 1.2452
epoch 1 iter 22600: loss = 1.3603,  smooth loss = 1.2603
epoch 1 iter 22650: loss = 1.3726,  smooth loss = 1.2554
epoch 1 iter 22700: loss = 1.1406,  smooth loss = 1.2589
epoch 1 iter 22750: loss = 1.3376,  smooth loss = 1.2668
epoch 1 iter 22800: loss = 1.2494,  smooth loss = 1.2540
epoch 1 iter 22850: loss = 1.3807,  smooth loss = 1.2681
epoch 1 iter 22900: loss = 1.2915,  smooth loss = 1.2527
epoch 1 iter 22950: loss = 1.2351,  smooth loss = 1.2468
epoch 1 iter 23000: loss = 1.1489,  smooth loss = 1.2569
average data time = 0.0040s, average running time = 0.8626s
epoch 1 iter 23000: eval loss = 0.6212,  ccr = 0.8894,  cwr = 0.7614,  ted = 1910.0000,  ned = 753.5238,  ted/w = 0.4608, 
Save model conclr-pretrain-vision-model_1_23000
epoch 1 iter 23050: loss = 1.2517,  smooth loss = 1.2802
epoch 1 iter 23100: loss = 1.3507,  smooth loss = 1.2572
epoch 1 iter 23150: loss = 1.1793,  smooth loss = 1.2571
epoch 1 iter 23200: loss = 1.2624,  smooth loss = 1.2546
epoch 1 iter 23250: loss = 1.2415,  smooth loss = 1.2619
epoch 1 iter 23300: loss = 1.3742,  smooth loss = 1.2412
epoch 1 iter 23350: loss = 0.9823,  smooth loss = 1.2364
epoch 1 iter 23400: loss = 1.4362,  smooth loss = 1.2508
epoch 1 iter 23450: loss = 1.1449,  smooth loss = 1.2533
epoch 1 iter 23500: loss = 1.2778,  smooth loss = 1.2613
epoch 1 iter 23550: loss = 1.1986,  smooth loss = 1.2537
epoch 1 iter 23600: loss = 1.1881,  smooth loss = 1.2430
epoch 1 iter 23650: loss = 1.4941,  smooth loss = 1.2419
epoch 1 iter 23700: loss = 1.1379,  smooth loss = 1.2398
epoch 1 iter 23750: loss = 1.0894,  smooth loss = 1.2232
epoch 1 iter 23800: loss = 1.0987,  smooth loss = 1.2467
epoch 1 iter 23850: loss = 1.3762,  smooth loss = 1.2482
epoch 1 iter 23900: loss = 1.2043,  smooth loss = 1.2437
epoch 1 iter 23950: loss = 1.3593,  smooth loss = 1.2355
epoch 1 iter 24000: loss = 1.1354,  smooth loss = 1.2382
average data time = 0.0039s, average running time = 0.8627s
epoch 1 iter 24000: eval loss = 0.5946,  ccr = 0.8908,  cwr = 0.7665,  ted = 1786.0000,  ned = 735.5477,  ted/w = 0.4309, 
Save model conclr-pretrain-vision-model_1_24000
epoch 1 iter 24050: loss = 1.1484,  smooth loss = 1.2340
epoch 1 iter 24100: loss = 1.0721,  smooth loss = 1.2245
epoch 1 iter 24150: loss = 1.1443,  smooth loss = 1.2446
epoch 1 iter 24200: loss = 1.1209,  smooth loss = 1.2173
epoch 1 iter 24250: loss = 1.2889,  smooth loss = 1.2170
epoch 1 iter 24300: loss = 1.2699,  smooth loss = 1.2340
epoch 1 iter 24350: loss = 1.2954,  smooth loss = 1.2215
epoch 1 iter 24400: loss = 1.2600,  smooth loss = 1.2141
epoch 1 iter 24450: loss = 1.3035,  smooth loss = 1.2315
epoch 1 iter 24500: loss = 1.0113,  smooth loss = 1.2290
epoch 1 iter 24550: loss = 1.2163,  smooth loss = 1.2335
epoch 1 iter 24600: loss = 1.1427,  smooth loss = 1.2318
epoch 1 iter 24650: loss = 1.0802,  smooth loss = 1.2212
epoch 1 iter 24700: loss = 1.1411,  smooth loss = 1.2113
epoch 1 iter 24750: loss = 1.2268,  smooth loss = 1.2251
epoch 1 iter 24800: loss = 1.2238,  smooth loss = 1.2201
epoch 1 iter 24850: loss = 1.0556,  smooth loss = 1.2028
epoch 1 iter 24900: loss = 1.2230,  smooth loss = 1.2192
epoch 1 iter 24950: loss = 1.3786,  smooth loss = 1.1999
epoch 1 iter 25000: loss = 1.1308,  smooth loss = 1.2031
average data time = 0.0038s, average running time = 0.8623s
epoch 1 iter 25000: eval loss = 0.6874,  ccr = 0.8817,  cwr = 0.7440,  ted = 1968.0000,  ned = 759.2051,  ted/w = 0.4748, 
Save model conclr-pretrain-vision-model_1_25000
epoch 1 iter 25050: loss = 1.3774,  smooth loss = 1.2064
epoch 1 iter 25100: loss = 1.2821,  smooth loss = 1.2011
epoch 1 iter 25150: loss = 1.3222,  smooth loss = 1.2209
epoch 1 iter 25200: loss = 1.2585,  smooth loss = 1.2145
epoch 1 iter 25250: loss = 1.2505,  smooth loss = 1.2105
epoch 1 iter 25300: loss = 1.2621,  smooth loss = 1.2264
epoch 1 iter 25350: loss = 1.3619,  smooth loss = 1.2326
epoch 1 iter 25400: loss = 1.1765,  smooth loss = 1.2248
epoch 1 iter 25450: loss = 1.2240,  smooth loss = 1.2099
epoch 1 iter 25500: loss = 1.1889,  smooth loss = 1.2094
epoch 1 iter 25550: loss = 1.3002,  smooth loss = 1.1975
epoch 1 iter 25600: loss = 1.2858,  smooth loss = 1.2256
epoch 1 iter 25650: loss = 1.3291,  smooth loss = 1.2237
epoch 1 iter 25700: loss = 1.1305,  smooth loss = 1.2103
epoch 1 iter 25750: loss = 1.0031,  smooth loss = 1.1978
epoch 1 iter 25800: loss = 1.2549,  smooth loss = 1.1862
epoch 1 iter 25850: loss = 0.9999,  smooth loss = 1.1882
epoch 1 iter 25900: loss = 1.3593,  smooth loss = 1.2012
epoch 1 iter 25950: loss = 1.3606,  smooth loss = 1.2043
epoch 1 iter 26000: loss = 1.3014,  smooth loss = 1.2054
average data time = 0.0037s, average running time = 0.8620s
epoch 1 iter 26000: eval loss = 0.6081,  ccr = 0.8910,  cwr = 0.7650,  ted = 1695.0000,  ned = 637.0136,  ted/w = 0.4089, 
Save model conclr-pretrain-vision-model_1_26000
epoch 1 iter 26050: loss = 1.2176,  smooth loss = 1.2057
epoch 1 iter 26100: loss = 1.1650,  smooth loss = 1.1988
epoch 1 iter 26150: loss = 1.1893,  smooth loss = 1.1821
epoch 1 iter 26200: loss = 1.2075,  smooth loss = 1.2060
epoch 1 iter 26250: loss = 1.2136,  smooth loss = 1.1945
epoch 1 iter 26300: loss = 1.1674,  smooth loss = 1.1741
epoch 1 iter 26350: loss = 1.1270,  smooth loss = 1.1896
epoch 1 iter 26400: loss = 1.2680,  smooth loss = 1.1869
epoch 1 iter 26450: loss = 1.3105,  smooth loss = 1.2019
epoch 1 iter 26500: loss = 1.1618,  smooth loss = 1.2057
epoch 1 iter 26550: loss = 1.2216,  smooth loss = 1.1956
epoch 1 iter 26600: loss = 1.1928,  smooth loss = 1.2089
epoch 1 iter 26650: loss = 1.1966,  smooth loss = 1.1988
epoch 1 iter 26700: loss = 1.2542,  smooth loss = 1.1996
epoch 1 iter 26750: loss = 1.3157,  smooth loss = 1.1998
epoch 1 iter 26800: loss = 1.3094,  smooth loss = 1.1992
epoch 1 iter 26850: loss = 1.2865,  smooth loss = 1.2168
epoch 1 iter 26900: loss = 1.1086,  smooth loss = 1.1758
epoch 1 iter 26950: loss = 1.1657,  smooth loss = 1.1865
epoch 1 iter 27000: loss = 1.4050,  smooth loss = 1.1803
average data time = 0.0036s, average running time = 0.8616s
epoch 1 iter 27000: eval loss = 0.6035,  ccr = 0.8892,  cwr = 0.7867,  ted = 1641.0000,  ned = 583.8369,  ted/w = 0.3959, 
Better model found at epoch 1, iter 27000 with accuracy value: 0.7867.
Save model conclr-pretrain-vision-model_1_27000
epoch 1 iter 27050: loss = 1.0960,  smooth loss = 1.2053
epoch 1 iter 27100: loss = 1.1799,  smooth loss = 1.1880
epoch 1 iter 27150: loss = 1.1681,  smooth loss = 1.1683
epoch 1 iter 27200: loss = 1.2947,  smooth loss = 1.1907
epoch 1 iter 27250: loss = 1.2322,  smooth loss = 1.1810
epoch 1 iter 27300: loss = 1.1114,  smooth loss = 1.1805
epoch 1 iter 27350: loss = 1.1760,  smooth loss = 1.1691
epoch 1 iter 27400: loss = 1.0162,  smooth loss = 1.1721
epoch 1 iter 27450: loss = 1.1420,  smooth loss = 1.1752
epoch 1 iter 27500: loss = 1.1883,  smooth loss = 1.1807
epoch 1 iter 27550: loss = 1.1608,  smooth loss = 1.1837
epoch 1 iter 27600: loss = 0.9807,  smooth loss = 1.1708
epoch 1 iter 27650: loss = 1.2892,  smooth loss = 1.1624
epoch 1 iter 27700: loss = 1.1944,  smooth loss = 1.1671
epoch 1 iter 27750: loss = 1.2152,  smooth loss = 1.1722
epoch 1 iter 27800: loss = 1.1835,  smooth loss = 1.1633
epoch 1 iter 27850: loss = 0.9804,  smooth loss = 1.1754
epoch 1 iter 27900: loss = 1.0313,  smooth loss = 1.1718
epoch 1 iter 27950: loss = 1.3449,  smooth loss = 1.1792
epoch 1 iter 28000: loss = 1.2141,  smooth loss = 1.1813
average data time = 0.0036s, average running time = 0.8613s
epoch 1 iter 28000: eval loss = 0.5768,  ccr = 0.8996,  cwr = 0.7870,  ted = 1509.0000,  ned = 605.9177,  ted/w = 0.3641, 
Better model found at epoch 1, iter 28000 with accuracy value: 0.7870.
Save model conclr-pretrain-vision-model_1_28000
epoch 1 iter 28050: loss = 1.1639,  smooth loss = 1.1846
epoch 1 iter 28100: loss = 1.2067,  smooth loss = 1.1736
epoch 1 iter 28150: loss = 1.0771,  smooth loss = 1.1741
epoch 1 iter 28200: loss = 1.1761,  smooth loss = 1.1676
epoch 1 iter 28250: loss = 1.1595,  smooth loss = 1.1902
epoch 1 iter 28300: loss = 1.1884,  smooth loss = 1.1807
epoch 1 iter 28350: loss = 1.1990,  smooth loss = 1.1709
epoch 1 iter 28400: loss = 1.1255,  smooth loss = 1.1616
epoch 1 iter 28450: loss = 0.9636,  smooth loss = 1.1532
epoch 1 iter 28500: loss = 1.1695,  smooth loss = 1.1601
epoch 1 iter 28550: loss = 1.2755,  smooth loss = 1.1625
epoch 1 iter 28600: loss = 1.1878,  smooth loss = 1.1510
epoch 1 iter 28650: loss = 1.2896,  smooth loss = 1.1595
epoch 1 iter 28700: loss = 1.1418,  smooth loss = 1.1670
epoch 1 iter 28750: loss = 1.3555,  smooth loss = 1.1799
epoch 1 iter 28800: loss = 1.0919,  smooth loss = 1.1737
epoch 1 iter 28850: loss = 1.0763,  smooth loss = 1.1698
epoch 1 iter 28900: loss = 1.1763,  smooth loss = 1.1753
epoch 1 iter 28950: loss = 1.3125,  smooth loss = 1.1543
epoch 1 iter 29000: loss = 1.2304,  smooth loss = 1.1591
average data time = 0.0035s, average running time = 0.8610s
epoch 1 iter 29000: eval loss = 0.5942,  ccr = 0.8927,  cwr = 0.7862,  ted = 1530.0000,  ned = 595.9061,  ted/w = 0.3691, 
Save model conclr-pretrain-vision-model_1_29000
epoch 1 iter 29050: loss = 1.1866,  smooth loss = 1.1631
epoch 1 iter 29100: loss = 1.1205,  smooth loss = 1.1677
epoch 1 iter 29150: loss = 1.2126,  smooth loss = 1.1711
epoch 1 iter 29200: loss = 1.1574,  smooth loss = 1.1553
epoch 1 iter 29250: loss = 1.1580,  smooth loss = 1.1626
epoch 1 iter 29300: loss = 1.1551,  smooth loss = 1.1721
epoch 1 iter 29350: loss = 1.2433,  smooth loss = 1.1816
epoch 1 iter 29400: loss = 1.1935,  smooth loss = 1.1742
epoch 1 iter 29450: loss = 1.0675,  smooth loss = 1.1660
epoch 1 iter 29500: loss = 0.9531,  smooth loss = 1.1641
epoch 1 iter 29550: loss = 1.1382,  smooth loss = 1.1547
epoch 1 iter 29600: loss = 1.2017,  smooth loss = 1.1743
epoch 1 iter 29650: loss = 1.3185,  smooth loss = 1.1692
epoch 1 iter 29700: loss = 1.1272,  smooth loss = 1.1736
epoch 1 iter 29750: loss = 1.2848,  smooth loss = 1.1636
epoch 1 iter 29800: loss = 1.1910,  smooth loss = 1.1615
epoch 1 iter 29850: loss = 1.1972,  smooth loss = 1.1572
epoch 1 iter 29900: loss = 1.2844,  smooth loss = 1.1606
epoch 1 iter 29950: loss = 1.2638,  smooth loss = 1.1623
epoch 1 iter 30000: loss = 1.1563,  smooth loss = 1.1565
average data time = 0.0034s, average running time = 0.8608s
epoch 1 iter 30000: eval loss = 0.6065,  ccr = 0.8820,  cwr = 0.7563,  ted = 1916.0000,  ned = 767.6929,  ted/w = 0.4622, 
Save model conclr-pretrain-vision-model_1_30000
epoch 1 iter 30050: loss = 1.1017,  smooth loss = 1.1497
epoch 1 iter 30100: loss = 1.2302,  smooth loss = 1.1501
epoch 1 iter 30150: loss = 1.0088,  smooth loss = 1.1428
epoch 1 iter 30200: loss = 1.0504,  smooth loss = 1.1581
epoch 1 iter 30250: loss = 1.1201,  smooth loss = 1.1475
epoch 1 iter 30300: loss = 1.1513,  smooth loss = 1.1485
epoch 1 iter 30350: loss = 1.1088,  smooth loss = 1.1554
epoch 1 iter 30400: loss = 1.4357,  smooth loss = 1.1528
epoch 1 iter 30450: loss = 1.1623,  smooth loss = 1.1396
epoch 1 iter 30500: loss = 1.3172,  smooth loss = 1.1646
epoch 1 iter 30550: loss = 1.2027,  smooth loss = 1.1407
epoch 1 iter 30600: loss = 1.1416,  smooth loss = 1.1476
epoch 1 iter 30650: loss = 1.0791,  smooth loss = 1.1346
epoch 1 iter 30700: loss = 1.0700,  smooth loss = 1.1358
epoch 1 iter 30750: loss = 1.1392,  smooth loss = 1.1420
epoch 1 iter 30800: loss = 1.1492,  smooth loss = 1.1262
epoch 1 iter 30850: loss = 1.1959,  smooth loss = 1.1310
epoch 1 iter 30900: loss = 1.1053,  smooth loss = 1.1399
epoch 1 iter 30950: loss = 1.2003,  smooth loss = 1.1569
epoch 1 iter 31000: loss = 1.1441,  smooth loss = 1.1395
average data time = 0.0034s, average running time = 0.8607s
epoch 1 iter 31000: eval loss = 0.6083,  ccr = 0.8984,  cwr = 0.7744,  ted = 1728.0000,  ned = 734.5541,  ted/w = 0.4169, 
Save model conclr-pretrain-vision-model_1_31000
epoch 1 iter 31050: loss = 1.0189,  smooth loss = 1.1457
epoch 1 iter 31100: loss = 1.0933,  smooth loss = 1.1552
epoch 1 iter 31150: loss = 1.0508,  smooth loss = 1.1573
epoch 1 iter 31200: loss = 1.0004,  smooth loss = 1.1561
epoch 1 iter 31250: loss = 1.1965,  smooth loss = 1.1426
epoch 1 iter 31300: loss = 1.1926,  smooth loss = 1.1378
epoch 1 iter 31350: loss = 1.3282,  smooth loss = 1.1375
epoch 1 iter 31400: loss = 1.2217,  smooth loss = 1.1330
epoch 1 iter 31450: loss = 1.1692,  smooth loss = 1.1321
epoch 1 iter 31500: loss = 1.1201,  smooth loss = 1.1314
epoch 1 iter 31550: loss = 0.9018,  smooth loss = 1.1397
epoch 1 iter 31600: loss = 1.0042,  smooth loss = 1.1350
epoch 1 iter 31650: loss = 1.1503,  smooth loss = 1.1279
epoch 1 iter 31700: loss = 1.2510,  smooth loss = 1.1518
epoch 1 iter 31750: loss = 1.0856,  smooth loss = 1.1505
epoch 1 iter 31800: loss = 0.9871,  smooth loss = 1.1291
epoch 1 iter 31850: loss = 1.0528,  smooth loss = 1.1317
epoch 1 iter 31900: loss = 1.2324,  smooth loss = 1.1622
epoch 1 iter 31950: loss = 1.2784,  smooth loss = 1.1636
epoch 1 iter 32000: loss = 1.2761,  smooth loss = 1.1401
average data time = 0.0033s, average running time = 0.8610s
epoch 1 iter 32000: eval loss = 0.6439,  ccr = 0.8865,  cwr = 0.7860,  ted = 1703.0000,  ned = 608.0582,  ted/w = 0.4109, 
Save model conclr-pretrain-vision-model_1_32000
epoch 1 iter 32050: loss = 1.2099,  smooth loss = 1.1568
epoch 1 iter 32100: loss = 0.9840,  smooth loss = 1.1345
epoch 1 iter 32150: loss = 1.1249,  smooth loss = 1.1425
epoch 1 iter 32200: loss = 1.1737,  smooth loss = 1.1454
epoch 1 iter 32250: loss = 0.9986,  smooth loss = 1.1284
epoch 1 iter 32300: loss = 1.1310,  smooth loss = 1.1288
epoch 1 iter 32350: loss = 1.1739,  smooth loss = 1.1173
epoch 1 iter 32400: loss = 1.2024,  smooth loss = 1.1169
epoch 1 iter 32450: loss = 0.9754,  smooth loss = 1.1117
epoch 1 iter 32500: loss = 1.1089,  smooth loss = 1.1081
epoch 1 iter 32550: loss = 1.2475,  smooth loss = 1.1119
epoch 1 iter 32600: loss = 0.9459,  smooth loss = 1.1276
epoch 1 iter 32650: loss = 1.0893,  smooth loss = 1.1500
epoch 1 iter 32700: loss = 1.0865,  smooth loss = 1.1150
epoch 1 iter 32750: loss = 0.9864,  smooth loss = 1.1327
epoch 1 iter 32800: loss = 1.0438,  smooth loss = 1.1296
epoch 1 iter 32850: loss = 1.0914,  smooth loss = 1.1226
epoch 1 iter 32900: loss = 1.2369,  smooth loss = 1.1368
epoch 1 iter 32950: loss = 1.0507,  smooth loss = 1.1319
epoch 1 iter 33000: loss = 1.0292,  smooth loss = 1.1208
average data time = 0.0033s, average running time = 0.8611s
epoch 1 iter 33000: eval loss = 0.6153,  ccr = 0.8910,  cwr = 0.7701,  ted = 1841.0000,  ned = 819.3174,  ted/w = 0.4441, 
Save model conclr-pretrain-vision-model_1_33000
epoch 1 iter 33050: loss = 1.1524,  smooth loss = 1.1164
epoch 1 iter 33100: loss = 1.4214,  smooth loss = 1.1220
epoch 1 iter 33150: loss = 1.2369,  smooth loss = 1.1259
epoch 1 iter 33200: loss = 0.9581,  smooth loss = 1.1224
epoch 1 iter 33250: loss = 1.0791,  smooth loss = 1.1005
epoch 1 iter 33300: loss = 0.9896,  smooth loss = 1.1179
epoch 1 iter 33350: loss = 1.1969,  smooth loss = 1.1111
epoch 1 iter 33400: loss = 0.9789,  smooth loss = 1.1103
epoch 1 iter 33450: loss = 1.0867,  smooth loss = 1.1073
epoch 1 iter 33500: loss = 1.1571,  smooth loss = 1.1121
epoch 1 iter 33550: loss = 1.0091,  smooth loss = 1.1223
epoch 1 iter 33600: loss = 0.9945,  smooth loss = 1.1276
epoch 1 iter 33650: loss = 1.0276,  smooth loss = 1.1361
epoch 1 iter 33700: loss = 1.1333,  smooth loss = 1.1341
epoch 1 iter 33750: loss = 1.1127,  smooth loss = 1.1156
epoch 1 iter 33800: loss = 1.0842,  smooth loss = 1.1137
epoch 1 iter 33850: loss = 0.9960,  smooth loss = 1.1201
epoch 1 iter 33900: loss = 1.0525,  smooth loss = 1.1023
epoch 1 iter 33950: loss = 1.1309,  smooth loss = 1.1088
epoch 1 iter 34000: loss = 1.1456,  smooth loss = 1.1329
average data time = 0.0032s, average running time = 0.8610s
epoch 1 iter 34000: eval loss = 0.5856,  ccr = 0.8970,  cwr = 0.7780,  ted = 1497.0000,  ned = 590.0126,  ted/w = 0.3612, 
Save model conclr-pretrain-vision-model_1_34000
epoch 1 iter 34050: loss = 1.0870,  smooth loss = 1.1308
epoch 1 iter 34100: loss = 1.0665,  smooth loss = 1.1273
epoch 1 iter 34150: loss = 1.1530,  smooth loss = 1.1157
epoch 1 iter 34200: loss = 1.1623,  smooth loss = 1.1207
epoch 1 iter 34250: loss = 1.2143,  smooth loss = 1.1279
epoch 1 iter 34300: loss = 1.1722,  smooth loss = 1.1159
epoch 1 iter 34350: loss = 1.1858,  smooth loss = 1.1204
epoch 1 iter 34400: loss = 0.9379,  smooth loss = 1.1128
epoch 1 iter 34450: loss = 1.0565,  smooth loss = 1.1177
epoch 1 iter 34500: loss = 1.1704,  smooth loss = 1.1131
epoch 1 iter 34550: loss = 1.0321,  smooth loss = 1.1002
epoch 1 iter 34600: loss = 1.0641,  smooth loss = 1.1208
epoch 1 iter 34650: loss = 1.0388,  smooth loss = 1.1067
epoch 1 iter 34700: loss = 1.0617,  smooth loss = 1.1234
epoch 1 iter 34750: loss = 1.0580,  smooth loss = 1.1158
epoch 1 iter 34800: loss = 1.1082,  smooth loss = 1.1292
epoch 1 iter 34850: loss = 1.2210,  smooth loss = 1.1094
epoch 1 iter 34900: loss = 1.0400,  smooth loss = 1.1095
epoch 1 iter 34950: loss = 1.2120,  smooth loss = 1.1213
epoch 1 iter 35000: loss = 1.0288,  smooth loss = 1.1164
average data time = 0.0032s, average running time = 0.8608s
epoch 1 iter 35000: eval loss = 0.6555,  ccr = 0.8967,  cwr = 0.7619,  ted = 1703.0000,  ned = 724.2665,  ted/w = 0.4109, 
Save model conclr-pretrain-vision-model_1_35000
epoch 1 iter 35050: loss = 1.1685,  smooth loss = 1.1196
epoch 1 iter 35100: loss = 0.9897,  smooth loss = 1.1020
epoch 1 iter 35150: loss = 1.1674,  smooth loss = 1.1048
epoch 1 iter 35200: loss = 1.1214,  smooth loss = 1.0959
epoch 1 iter 35250: loss = 1.0370,  smooth loss = 1.1179
epoch 1 iter 35300: loss = 1.1523,  smooth loss = 1.1177
epoch 1 iter 35350: loss = 1.1842,  smooth loss = 1.1302
epoch 1 iter 35400: loss = 0.9948,  smooth loss = 1.1123
epoch 1 iter 35450: loss = 1.2674,  smooth loss = 1.1121
epoch 1 iter 35500: loss = 1.0116,  smooth loss = 1.1153
epoch 1 iter 35550: loss = 1.1006,  smooth loss = 1.0937
epoch 1 iter 35600: loss = 1.0363,  smooth loss = 1.0978
epoch 1 iter 35650: loss = 1.2125,  smooth loss = 1.1026
epoch 1 iter 35700: loss = 1.0795,  smooth loss = 1.0925
epoch 1 iter 35750: loss = 1.1644,  smooth loss = 1.1027
epoch 1 iter 35800: loss = 0.9544,  smooth loss = 1.1115
epoch 1 iter 35850: loss = 1.1922,  smooth loss = 1.1096
epoch 1 iter 35900: loss = 1.0335,  smooth loss = 1.0981
epoch 1 iter 35950: loss = 1.0766,  smooth loss = 1.1073
epoch 1 iter 36000: loss = 1.0604,  smooth loss = 1.1301
average data time = 0.0031s, average running time = 0.8605s
epoch 1 iter 36000: eval loss = 0.6341,  ccr = 0.8869,  cwr = 0.7561,  ted = 1679.0000,  ned = 689.5290,  ted/w = 0.4051, 
Save model conclr-pretrain-vision-model_1_36000
epoch 1 iter 36050: loss = 1.2207,  smooth loss = 1.1288
epoch 1 iter 36100: loss = 1.2258,  smooth loss = 1.1005
epoch 1 iter 36150: loss = 0.9234,  smooth loss = 1.0935
epoch 1 iter 36200: loss = 1.1468,  smooth loss = 1.1156
epoch 1 iter 36250: loss = 1.1080,  smooth loss = 1.1100
epoch 1 iter 36300: loss = 1.0658,  smooth loss = 1.0984
epoch 2 iter 36350: loss = 1.1044,  smooth loss = 1.0999
epoch 2 iter 36400: loss = 1.1625,  smooth loss = 1.0995
epoch 2 iter 36450: loss = 1.2500,  smooth loss = 1.1033
epoch 2 iter 36500: loss = 1.0747,  smooth loss = 1.0904
epoch 2 iter 36550: loss = 1.0982,  smooth loss = 1.0900
epoch 2 iter 36600: loss = 1.0272,  smooth loss = 1.0983
epoch 2 iter 36650: loss = 0.7924,  smooth loss = 1.0981
epoch 2 iter 36700: loss = 1.1074,  smooth loss = 1.1083
epoch 2 iter 36750: loss = 1.2808,  smooth loss = 1.1022
epoch 2 iter 36800: loss = 1.2617,  smooth loss = 1.0905
epoch 2 iter 36850: loss = 0.9389,  smooth loss = 1.0897
epoch 2 iter 36900: loss = 1.0802,  smooth loss = 1.0865
epoch 2 iter 36950: loss = 1.1798,  smooth loss = 1.1076
epoch 2 iter 37000: loss = 0.9846,  smooth loss = 1.0946
average data time = 0.0037s, average running time = 0.8603s
epoch 2 iter 37000: eval loss = 0.5773,  ccr = 0.8920,  cwr = 0.7976,  ted = 1531.0000,  ned = 517.6795,  ted/w = 0.3694, 
Better model found at epoch 2, iter 37000 with accuracy value: 0.7976.
Save model conclr-pretrain-vision-model_2_37000
epoch 2 iter 37050: loss = 1.2679,  smooth loss = 1.0849
epoch 2 iter 37100: loss = 0.9075,  smooth loss = 1.0948
epoch 2 iter 37150: loss = 1.1155,  smooth loss = 1.0772
epoch 2 iter 37200: loss = 1.1535,  smooth loss = 1.0872
epoch 2 iter 37250: loss = 1.1577,  smooth loss = 1.0863
epoch 2 iter 37300: loss = 1.0728,  smooth loss = 1.0861
epoch 2 iter 37350: loss = 1.1232,  smooth loss = 1.0848
epoch 2 iter 37400: loss = 0.9719,  smooth loss = 1.0972
epoch 2 iter 37450: loss = 1.1390,  smooth loss = 1.0819
epoch 2 iter 37500: loss = 0.9889,  smooth loss = 1.0931
epoch 2 iter 37550: loss = 1.0308,  smooth loss = 1.1107
epoch 2 iter 37600: loss = 0.9291,  smooth loss = 1.0976
epoch 2 iter 37650: loss = 1.0697,  smooth loss = 1.1011
epoch 2 iter 37700: loss = 1.0234,  smooth loss = 1.0891
epoch 2 iter 37750: loss = 1.1730,  smooth loss = 1.0857
epoch 2 iter 37800: loss = 1.0430,  smooth loss = 1.0703
epoch 2 iter 37850: loss = 1.2383,  smooth loss = 1.0826
epoch 2 iter 37900: loss = 1.0964,  smooth loss = 1.1040
epoch 2 iter 37950: loss = 0.9882,  smooth loss = 1.0757
epoch 2 iter 38000: loss = 1.1306,  smooth loss = 1.0952
average data time = 0.0036s, average running time = 0.8602s
epoch 2 iter 38000: eval loss = 0.6022,  ccr = 0.9024,  cwr = 0.7978,  ted = 1409.0000,  ned = 490.7039,  ted/w = 0.3399, 
Better model found at epoch 2, iter 38000 with accuracy value: 0.7978.
Save model conclr-pretrain-vision-model_2_38000
epoch 2 iter 38050: loss = 1.1394,  smooth loss = 1.0955
epoch 2 iter 38100: loss = 1.0613,  smooth loss = 1.0966
epoch 2 iter 38150: loss = 1.1123,  smooth loss = 1.0931
epoch 2 iter 38200: loss = 1.0976,  smooth loss = 1.0768
epoch 2 iter 38250: loss = 0.9959,  smooth loss = 1.0740
epoch 2 iter 38300: loss = 1.0806,  smooth loss = 1.0772
epoch 2 iter 38350: loss = 1.0344,  smooth loss = 1.0847
epoch 2 iter 38400: loss = 0.9298,  smooth loss = 1.0911
epoch 2 iter 38450: loss = 1.0221,  smooth loss = 1.0962
epoch 2 iter 38500: loss = 1.2906,  smooth loss = 1.0915
epoch 2 iter 38550: loss = 1.0331,  smooth loss = 1.0937
epoch 2 iter 38600: loss = 1.1039,  smooth loss = 1.0830
epoch 2 iter 38650: loss = 1.2641,  smooth loss = 1.0856
epoch 2 iter 38700: loss = 1.1809,  smooth loss = 1.0787
epoch 2 iter 38750: loss = 0.9809,  smooth loss = 1.0796
epoch 2 iter 38800: loss = 1.1280,  smooth loss = 1.0879
epoch 2 iter 38850: loss = 1.1834,  smooth loss = 1.0852
epoch 2 iter 38900: loss = 0.9371,  smooth loss = 1.0908
epoch 2 iter 38950: loss = 1.0857,  smooth loss = 1.0866
epoch 2 iter 39000: loss = 1.0445,  smooth loss = 1.0859
average data time = 0.0036s, average running time = 0.8602s
epoch 2 iter 39000: eval loss = 0.6088,  ccr = 0.9004,  cwr = 0.7920,  ted = 1541.0000,  ned = 585.9726,  ted/w = 0.3718, 
Save model conclr-pretrain-vision-model_2_39000
epoch 2 iter 39050: loss = 1.0961,  smooth loss = 1.0768
epoch 2 iter 39100: loss = 1.0274,  smooth loss = 1.0763
epoch 2 iter 39150: loss = 1.0313,  smooth loss = 1.0688
epoch 2 iter 39200: loss = 1.0908,  smooth loss = 1.0681
epoch 2 iter 39250: loss = 0.9873,  smooth loss = 1.0582
epoch 2 iter 39300: loss = 1.1226,  smooth loss = 1.0825
epoch 2 iter 39350: loss = 0.9516,  smooth loss = 1.0776
epoch 2 iter 39400: loss = 1.1085,  smooth loss = 1.0934
epoch 2 iter 39450: loss = 1.2045,  smooth loss = 1.0960
epoch 2 iter 39500: loss = 1.2413,  smooth loss = 1.0955
epoch 2 iter 39550: loss = 1.2154,  smooth loss = 1.0805
epoch 2 iter 39600: loss = 0.9954,  smooth loss = 1.0787
epoch 2 iter 39650: loss = 1.0035,  smooth loss = 1.0793
epoch 2 iter 39700: loss = 0.9561,  smooth loss = 1.0778
epoch 2 iter 39750: loss = 1.2535,  smooth loss = 1.0784
epoch 2 iter 39800: loss = 1.1147,  smooth loss = 1.0858
epoch 2 iter 39850: loss = 0.9764,  smooth loss = 1.0704
epoch 2 iter 39900: loss = 1.1391,  smooth loss = 1.0743
epoch 2 iter 39950: loss = 1.1085,  smooth loss = 1.0720
epoch 2 iter 40000: loss = 1.1316,  smooth loss = 1.0733
average data time = 0.0035s, average running time = 0.8602s
epoch 2 iter 40000: eval loss = 0.5307,  ccr = 0.9166,  cwr = 0.8104,  ted = 1335.0000,  ned = 516.3495,  ted/w = 0.3221, 
Better model found at epoch 2, iter 40000 with accuracy value: 0.8104.
Save model conclr-pretrain-vision-model_2_40000
epoch 2 iter 40050: loss = 1.0877,  smooth loss = 1.0817
epoch 2 iter 40100: loss = 1.3248,  smooth loss = 1.0765
epoch 2 iter 40150: loss = 1.0322,  smooth loss = 1.0824
epoch 2 iter 40200: loss = 0.9578,  smooth loss = 1.0715
epoch 2 iter 40250: loss = 1.0236,  smooth loss = 1.0665
epoch 2 iter 40300: loss = 0.9376,  smooth loss = 1.0669
epoch 2 iter 40350: loss = 1.0634,  smooth loss = 1.0706
epoch 2 iter 40400: loss = 1.1456,  smooth loss = 1.0712
epoch 2 iter 40450: loss = 1.2548,  smooth loss = 1.0749
epoch 2 iter 40500: loss = 1.0520,  smooth loss = 1.0843
epoch 2 iter 40550: loss = 0.9375,  smooth loss = 1.0746
epoch 2 iter 40600: loss = 1.0722,  smooth loss = 1.0672
epoch 2 iter 40650: loss = 1.0677,  smooth loss = 1.0632
epoch 2 iter 40700: loss = 1.0545,  smooth loss = 1.0625
epoch 2 iter 40750: loss = 1.1405,  smooth loss = 1.0714
epoch 2 iter 40800: loss = 0.9702,  smooth loss = 1.0726
epoch 2 iter 40850: loss = 0.9968,  smooth loss = 1.0593
epoch 2 iter 40900: loss = 1.0919,  smooth loss = 1.0636
epoch 2 iter 40950: loss = 1.1933,  smooth loss = 1.0687
epoch 2 iter 41000: loss = 0.9967,  smooth loss = 1.0503
average data time = 0.0035s, average running time = 0.8601s
epoch 2 iter 41000: eval loss = 0.5506,  ccr = 0.9100,  cwr = 0.8092,  ted = 1407.0000,  ned = 525.4091,  ted/w = 0.3394, 
Save model conclr-pretrain-vision-model_2_41000
epoch 2 iter 41050: loss = 1.0209,  smooth loss = 1.0681
epoch 2 iter 41100: loss = 1.0203,  smooth loss = 1.0687
epoch 2 iter 41150: loss = 1.0204,  smooth loss = 1.0538
epoch 2 iter 41200: loss = 1.0866,  smooth loss = 1.0648
epoch 2 iter 41250: loss = 1.1427,  smooth loss = 1.0901
epoch 2 iter 41300: loss = 0.9428,  smooth loss = 1.0683
epoch 2 iter 41350: loss = 1.0963,  smooth loss = 1.0663
epoch 2 iter 41400: loss = 1.3690,  smooth loss = 1.0713
epoch 2 iter 41450: loss = 1.1601,  smooth loss = 1.0704
epoch 2 iter 41500: loss = 1.1452,  smooth loss = 1.0705
epoch 2 iter 41550: loss = 1.0267,  smooth loss = 1.0691
epoch 2 iter 41600: loss = 0.8921,  smooth loss = 1.0558
epoch 2 iter 41650: loss = 1.0194,  smooth loss = 1.0539
epoch 2 iter 41700: loss = 1.1709,  smooth loss = 1.0694
epoch 2 iter 41750: loss = 0.9398,  smooth loss = 1.0606
epoch 2 iter 41800: loss = 0.9797,  smooth loss = 1.0674
epoch 2 iter 41850: loss = 1.0855,  smooth loss = 1.0772
epoch 2 iter 41900: loss = 1.1101,  smooth loss = 1.0819
epoch 2 iter 41950: loss = 1.2160,  smooth loss = 1.0744
epoch 2 iter 42000: loss = 1.0062,  smooth loss = 1.0714
average data time = 0.0034s, average running time = 0.8601s
epoch 2 iter 42000: eval loss = 0.5287,  ccr = 0.9163,  cwr = 0.8273,  ted = 1298.0000,  ned = 464.6040,  ted/w = 0.3131, 
Better model found at epoch 2, iter 42000 with accuracy value: 0.8273.
Save model conclr-pretrain-vision-model_2_42000
epoch 2 iter 42050: loss = 1.0621,  smooth loss = 1.0838
epoch 2 iter 42100: loss = 1.1630,  smooth loss = 1.0583
epoch 2 iter 42150: loss = 0.9273,  smooth loss = 1.0595
epoch 2 iter 42200: loss = 1.0907,  smooth loss = 1.0463
epoch 2 iter 42250: loss = 1.3161,  smooth loss = 1.0610
epoch 2 iter 42300: loss = 1.2290,  smooth loss = 1.0754
epoch 2 iter 42350: loss = 1.0068,  smooth loss = 1.0554
epoch 2 iter 42400: loss = 1.1936,  smooth loss = 1.0717
epoch 2 iter 42450: loss = 1.1128,  smooth loss = 1.0561
epoch 2 iter 42500: loss = 0.9331,  smooth loss = 1.0666
epoch 2 iter 42550: loss = 1.0565,  smooth loss = 1.0679
epoch 2 iter 42600: loss = 1.0946,  smooth loss = 1.0819
epoch 2 iter 42650: loss = 1.1433,  smooth loss = 1.0676
epoch 2 iter 42700: loss = 0.9387,  smooth loss = 1.0719
epoch 2 iter 42750: loss = 1.0277,  smooth loss = 1.0609
epoch 2 iter 42800: loss = 0.9733,  smooth loss = 1.0677
epoch 2 iter 42850: loss = 0.9615,  smooth loss = 1.0721
epoch 2 iter 42900: loss = 0.9965,  smooth loss = 1.0686
epoch 2 iter 42950: loss = 1.0397,  smooth loss = 1.0629
epoch 2 iter 43000: loss = 1.0321,  smooth loss = 1.0627
average data time = 0.0034s, average running time = 0.8600s
epoch 2 iter 43000: eval loss = 0.6137,  ccr = 0.8860,  cwr = 0.7426,  ted = 1893.0000,  ned = 759.8160,  ted/w = 0.4567, 
Save model conclr-pretrain-vision-model_2_43000
epoch 2 iter 43050: loss = 1.0395,  smooth loss = 1.0824
epoch 2 iter 43100: loss = 1.0186,  smooth loss = 1.0637
epoch 2 iter 43150: loss = 1.0133,  smooth loss = 1.0625
epoch 2 iter 43200: loss = 1.1614,  smooth loss = 1.0464
epoch 2 iter 43250: loss = 1.1489,  smooth loss = 1.0590
epoch 2 iter 43300: loss = 1.0154,  smooth loss = 1.0741
epoch 2 iter 43350: loss = 1.1894,  smooth loss = 1.0696
epoch 2 iter 43400: loss = 1.0819,  smooth loss = 1.0639
epoch 2 iter 43450: loss = 1.0617,  smooth loss = 1.0513
epoch 2 iter 43500: loss = 1.0455,  smooth loss = 1.0612
epoch 2 iter 43550: loss = 1.1499,  smooth loss = 1.0720
epoch 2 iter 43600: loss = 1.0568,  smooth loss = 1.0642
epoch 2 iter 43650: loss = 1.1103,  smooth loss = 1.0568
epoch 2 iter 43700: loss = 1.1056,  smooth loss = 1.0468
epoch 2 iter 43750: loss = 0.9814,  smooth loss = 1.0412
epoch 2 iter 43800: loss = 1.0338,  smooth loss = 1.0583
epoch 2 iter 43850: loss = 1.0260,  smooth loss = 1.0603
epoch 2 iter 43900: loss = 1.2319,  smooth loss = 1.0702
epoch 2 iter 43950: loss = 1.1121,  smooth loss = 1.0692
epoch 2 iter 44000: loss = 1.3247,  smooth loss = 1.0608
average data time = 0.0033s, average running time = 0.8599s
epoch 2 iter 44000: eval loss = 0.6405,  ccr = 0.8860,  cwr = 0.7674,  ted = 1894.0000,  ned = 737.4417,  ted/w = 0.4569, 
Save model conclr-pretrain-vision-model_2_44000
epoch 2 iter 44050: loss = 1.0338,  smooth loss = 1.0658
epoch 2 iter 44100: loss = 1.1161,  smooth loss = 1.0449
epoch 2 iter 44150: loss = 0.9912,  smooth loss = 1.0505
epoch 2 iter 44200: loss = 1.1435,  smooth loss = 1.0504
epoch 2 iter 44250: loss = 1.1545,  smooth loss = 1.0598
epoch 2 iter 44300: loss = 1.2271,  smooth loss = 1.0454
epoch 2 iter 44350: loss = 1.0342,  smooth loss = 1.0361
epoch 2 iter 44400: loss = 1.1153,  smooth loss = 1.0521
epoch 2 iter 44450: loss = 0.9995,  smooth loss = 1.0424
epoch 2 iter 44500: loss = 0.9617,  smooth loss = 1.0584
epoch 2 iter 44550: loss = 1.0874,  smooth loss = 1.0590
epoch 2 iter 44600: loss = 1.0746,  smooth loss = 1.0543
epoch 2 iter 44650: loss = 1.1515,  smooth loss = 1.0644
epoch 2 iter 44700: loss = 1.1701,  smooth loss = 1.0645
epoch 2 iter 44750: loss = 1.0372,  smooth loss = 1.0592
epoch 2 iter 44800: loss = 1.0607,  smooth loss = 1.0695
epoch 2 iter 44850: loss = 1.0276,  smooth loss = 1.0420
epoch 2 iter 44900: loss = 1.0359,  smooth loss = 1.0434
epoch 2 iter 44950: loss = 1.0615,  smooth loss = 1.0660
epoch 2 iter 45000: loss = 1.1469,  smooth loss = 1.0444
average data time = 0.0033s, average running time = 0.8598s
epoch 2 iter 45000: eval loss = 0.5894,  ccr = 0.9133,  cwr = 0.7862,  ted = 1414.0000,  ned = 601.1494,  ted/w = 0.3411, 
Save model conclr-pretrain-vision-model_2_45000
epoch 2 iter 45050: loss = 0.9827,  smooth loss = 1.0653
epoch 2 iter 45100: loss = 1.0156,  smooth loss = 1.0756
epoch 2 iter 45150: loss = 1.0796,  smooth loss = 1.0714
epoch 2 iter 45200: loss = 1.0476,  smooth loss = 1.0621
epoch 2 iter 45250: loss = 1.0530,  smooth loss = 1.0571
epoch 2 iter 45300: loss = 1.0126,  smooth loss = 1.0483
epoch 2 iter 45350: loss = 1.1122,  smooth loss = 1.0544
epoch 2 iter 45400: loss = 1.2332,  smooth loss = 1.0581
epoch 2 iter 45450: loss = 0.9982,  smooth loss = 1.0551
epoch 2 iter 45500: loss = 1.1341,  smooth loss = 1.0523
epoch 2 iter 45550: loss = 1.0496,  smooth loss = 1.0533
epoch 2 iter 45600: loss = 1.1383,  smooth loss = 1.0473
epoch 2 iter 45650: loss = 1.0092,  smooth loss = 1.0630
epoch 2 iter 45700: loss = 1.0845,  smooth loss = 1.0686
epoch 2 iter 45750: loss = 1.2094,  smooth loss = 1.0641
epoch 2 iter 45800: loss = 0.9424,  smooth loss = 1.0503
epoch 2 iter 45850: loss = 0.9547,  smooth loss = 1.0408
epoch 2 iter 45900: loss = 1.0040,  smooth loss = 1.0524
epoch 2 iter 45950: loss = 0.9181,  smooth loss = 1.0425
epoch 2 iter 46000: loss = 1.0226,  smooth loss = 1.0443
average data time = 0.0033s, average running time = 0.8597s
epoch 2 iter 46000: eval loss = 0.6090,  ccr = 0.8954,  cwr = 0.7698,  ted = 1640.0000,  ned = 674.3082,  ted/w = 0.3957, 
Save model conclr-pretrain-vision-model_2_46000
epoch 2 iter 46050: loss = 1.0932,  smooth loss = 1.0626
epoch 2 iter 46100: loss = 0.9918,  smooth loss = 1.0370
epoch 2 iter 46150: loss = 0.9724,  smooth loss = 1.0336
epoch 2 iter 46200: loss = 0.8598,  smooth loss = 1.0453
epoch 2 iter 46250: loss = 1.0321,  smooth loss = 1.0492
epoch 2 iter 46300: loss = 1.0514,  smooth loss = 1.0339
epoch 2 iter 46350: loss = 1.0028,  smooth loss = 1.0545
epoch 2 iter 46400: loss = 0.9905,  smooth loss = 1.0507
epoch 2 iter 46450: loss = 0.9341,  smooth loss = 1.0437
epoch 2 iter 46500: loss = 1.0692,  smooth loss = 1.0398
epoch 2 iter 46550: loss = 0.9302,  smooth loss = 1.0347
epoch 2 iter 46600: loss = 0.9384,  smooth loss = 1.0399
epoch 2 iter 46650: loss = 1.0442,  smooth loss = 1.0400
epoch 2 iter 46700: loss = 0.9646,  smooth loss = 1.0456
epoch 2 iter 46750: loss = 1.0181,  smooth loss = 1.0512
epoch 2 iter 46800: loss = 1.1184,  smooth loss = 1.0538
epoch 2 iter 46850: loss = 0.9968,  smooth loss = 1.0505
epoch 2 iter 46900: loss = 1.1907,  smooth loss = 1.0399
epoch 2 iter 46950: loss = 1.0451,  smooth loss = 1.0296
epoch 2 iter 47000: loss = 1.0074,  smooth loss = 1.0428
average data time = 0.0032s, average running time = 0.8595s
epoch 2 iter 47000: eval loss = 0.5747,  ccr = 0.9032,  cwr = 0.8164,  ted = 1492.0000,  ned = 622.1676,  ted/w = 0.3600, 
Save model conclr-pretrain-vision-model_2_47000
epoch 2 iter 47050: loss = 0.8945,  smooth loss = 1.0344
epoch 2 iter 47100: loss = 0.9360,  smooth loss = 1.0447
epoch 2 iter 47150: loss = 1.0075,  smooth loss = 1.0505
epoch 2 iter 47200: loss = 1.0532,  smooth loss = 1.0455
epoch 2 iter 47250: loss = 0.9954,  smooth loss = 1.0493
epoch 2 iter 47300: loss = 1.1434,  smooth loss = 1.0348
epoch 2 iter 47350: loss = 0.8571,  smooth loss = 1.0349
epoch 2 iter 47400: loss = 1.0828,  smooth loss = 1.0381
epoch 2 iter 47450: loss = 0.9777,  smooth loss = 1.0383
epoch 2 iter 47500: loss = 0.9055,  smooth loss = 1.0416
epoch 2 iter 47550: loss = 1.0903,  smooth loss = 1.0381
epoch 2 iter 47600: loss = 0.9738,  smooth loss = 1.0346
epoch 2 iter 47650: loss = 1.1219,  smooth loss = 1.0316
epoch 2 iter 47700: loss = 1.1078,  smooth loss = 1.0244
epoch 2 iter 47750: loss = 0.8929,  smooth loss = 1.0394
epoch 2 iter 47800: loss = 1.0824,  smooth loss = 1.0362
epoch 2 iter 47850: loss = 1.1157,  smooth loss = 1.0172
epoch 2 iter 47900: loss = 1.0791,  smooth loss = 1.0301
epoch 2 iter 47950: loss = 0.9539,  smooth loss = 1.0291
epoch 2 iter 48000: loss = 1.0049,  smooth loss = 1.0292
average data time = 0.0032s, average running time = 0.8594s
epoch 2 iter 48000: eval loss = 0.6210,  ccr = 0.9041,  cwr = 0.8205,  ted = 1408.0000,  ned = 525.5921,  ted/w = 0.3397, 
Save model conclr-pretrain-vision-model_2_48000
epoch 2 iter 48050: loss = 1.0491,  smooth loss = 1.0473
epoch 2 iter 48100: loss = 1.0643,  smooth loss = 1.0507
epoch 2 iter 48150: loss = 1.0675,  smooth loss = 1.0581
epoch 2 iter 48200: loss = 1.0886,  smooth loss = 1.0447
epoch 2 iter 48250: loss = 1.0907,  smooth loss = 1.0378
epoch 2 iter 48300: loss = 0.9769,  smooth loss = 1.0473
epoch 2 iter 48350: loss = 1.0373,  smooth loss = 1.0367
epoch 2 iter 48400: loss = 1.0198,  smooth loss = 1.0375
epoch 2 iter 48450: loss = 1.0370,  smooth loss = 1.0472
epoch 2 iter 48500: loss = 1.0803,  smooth loss = 1.0557
epoch 2 iter 48550: loss = 0.9201,  smooth loss = 1.0375
epoch 2 iter 48600: loss = 0.8947,  smooth loss = 1.0295
epoch 2 iter 48650: loss = 0.9770,  smooth loss = 1.0390
epoch 2 iter 48700: loss = 1.1545,  smooth loss = 1.0317
epoch 2 iter 48750: loss = 0.9544,  smooth loss = 1.0469
epoch 2 iter 48800: loss = 1.1542,  smooth loss = 1.0466
epoch 2 iter 48850: loss = 1.0307,  smooth loss = 1.0309
epoch 2 iter 48900: loss = 1.1422,  smooth loss = 1.0342
epoch 2 iter 48950: loss = 1.0170,  smooth loss = 1.0396
epoch 2 iter 49000: loss = 1.1050,  smooth loss = 1.0408
average data time = 0.0032s, average running time = 0.8593s
epoch 2 iter 49000: eval loss = 0.5787,  ccr = 0.9038,  cwr = 0.8142,  ted = 1389.0000,  ned = 573.7371,  ted/w = 0.3351, 
Save model conclr-pretrain-vision-model_2_49000
epoch 2 iter 49050: loss = 0.9960,  smooth loss = 1.0334
epoch 2 iter 49100: loss = 1.2099,  smooth loss = 1.0443
epoch 2 iter 49150: loss = 1.1956,  smooth loss = 1.0292
epoch 2 iter 49200: loss = 1.1325,  smooth loss = 1.0379
epoch 2 iter 49250: loss = 1.1355,  smooth loss = 1.0455
epoch 2 iter 49300: loss = 1.1047,  smooth loss = 1.0417
epoch 2 iter 49350: loss = 1.2193,  smooth loss = 1.0489
epoch 2 iter 49400: loss = 1.0453,  smooth loss = 1.0500
epoch 2 iter 49450: loss = 1.0589,  smooth loss = 1.0293
epoch 2 iter 49500: loss = 0.9480,  smooth loss = 1.0174
epoch 2 iter 49550: loss = 1.1735,  smooth loss = 1.0412
epoch 2 iter 49600: loss = 0.9282,  smooth loss = 1.0300
epoch 2 iter 49650: loss = 1.2117,  smooth loss = 1.0339
epoch 2 iter 49700: loss = 1.0759,  smooth loss = 1.0353
epoch 2 iter 49750: loss = 0.9032,  smooth loss = 1.0406
epoch 2 iter 49800: loss = 1.0140,  smooth loss = 1.0391
epoch 2 iter 49850: loss = 1.2120,  smooth loss = 1.0177
epoch 2 iter 49900: loss = 1.0551,  smooth loss = 1.0315
epoch 2 iter 49950: loss = 1.0320,  smooth loss = 1.0267
epoch 2 iter 50000: loss = 0.8163,  smooth loss = 1.0162
average data time = 0.0031s, average running time = 0.8592s
epoch 2 iter 50000: eval loss = 0.6211,  ccr = 0.9045,  cwr = 0.7877,  ted = 1586.0000,  ned = 737.2088,  ted/w = 0.3826, 
Save model conclr-pretrain-vision-model_2_50000
epoch 2 iter 50050: loss = 1.0708,  smooth loss = 1.0301
epoch 2 iter 50100: loss = 1.1154,  smooth loss = 1.0431
epoch 2 iter 50150: loss = 1.0188,  smooth loss = 1.0395
epoch 2 iter 50200: loss = 1.0002,  smooth loss = 1.0176
epoch 2 iter 50250: loss = 1.0392,  smooth loss = 1.0206
epoch 2 iter 50300: loss = 1.0646,  smooth loss = 1.0348
epoch 2 iter 50350: loss = 0.9840,  smooth loss = 1.0452
epoch 2 iter 50400: loss = 0.9877,  smooth loss = 1.0445
epoch 2 iter 50450: loss = 0.9690,  smooth loss = 1.0340
epoch 2 iter 50500: loss = 1.0365,  smooth loss = 1.0324
epoch 2 iter 50550: loss = 1.0657,  smooth loss = 1.0311
epoch 2 iter 50600: loss = 0.9762,  smooth loss = 1.0316
epoch 2 iter 50650: loss = 1.1287,  smooth loss = 1.0289
epoch 2 iter 50700: loss = 0.9472,  smooth loss = 1.0301
epoch 2 iter 50750: loss = 1.0878,  smooth loss = 1.0185
epoch 2 iter 50800: loss = 1.1657,  smooth loss = 1.0155
epoch 2 iter 50850: loss = 0.9393,  smooth loss = 1.0252
epoch 2 iter 50900: loss = 0.9807,  smooth loss = 1.0266
epoch 2 iter 50950: loss = 1.0123,  smooth loss = 1.0410
epoch 2 iter 51000: loss = 0.9290,  smooth loss = 1.0319
average data time = 0.0031s, average running time = 0.8590s
epoch 2 iter 51000: eval loss = 0.6328,  ccr = 0.9060,  cwr = 0.7780,  ted = 1527.0000,  ned = 612.9404,  ted/w = 0.3684, 
Save model conclr-pretrain-vision-model_2_51000
epoch 2 iter 51050: loss = 1.0597,  smooth loss = 1.0392
epoch 2 iter 51100: loss = 0.9829,  smooth loss = 1.0484
epoch 2 iter 51150: loss = 1.2298,  smooth loss = 1.0419
epoch 2 iter 51200: loss = 1.0071,  smooth loss = 1.0125
epoch 2 iter 51250: loss = 0.9398,  smooth loss = 1.0108
epoch 2 iter 51300: loss = 1.1245,  smooth loss = 1.0144
epoch 2 iter 51350: loss = 1.0902,  smooth loss = 1.0225
epoch 2 iter 51400: loss = 1.1913,  smooth loss = 1.0359
epoch 2 iter 51450: loss = 1.0116,  smooth loss = 1.0404
epoch 2 iter 51500: loss = 0.9997,  smooth loss = 1.0383
epoch 2 iter 51550: loss = 1.2371,  smooth loss = 1.0449
epoch 2 iter 51600: loss = 0.9504,  smooth loss = 1.0402
epoch 2 iter 51650: loss = 1.0381,  smooth loss = 1.0412
epoch 2 iter 51700: loss = 1.0619,  smooth loss = 1.0350
epoch 2 iter 51750: loss = 1.1066,  smooth loss = 1.0268
epoch 2 iter 51800: loss = 0.9976,  smooth loss = 1.0262
epoch 2 iter 51850: loss = 1.0043,  smooth loss = 1.0058
epoch 2 iter 51900: loss = 1.1458,  smooth loss = 1.0136
epoch 2 iter 51950: loss = 0.9336,  smooth loss = 1.0149
epoch 2 iter 52000: loss = 1.0687,  smooth loss = 1.0117
average data time = 0.0031s, average running time = 0.8590s
epoch 2 iter 52000: eval loss = 0.5860,  ccr = 0.8936,  cwr = 0.7826,  ted = 1483.0000,  ned = 570.8900,  ted/w = 0.3578, 
Save model conclr-pretrain-vision-model_2_52000
epoch 2 iter 52050: loss = 0.9966,  smooth loss = 1.0307
epoch 2 iter 52100: loss = 1.0498,  smooth loss = 1.0250
epoch 2 iter 52150: loss = 1.0644,  smooth loss = 1.0045
epoch 2 iter 52200: loss = 1.0678,  smooth loss = 1.0387
epoch 2 iter 52250: loss = 1.0740,  smooth loss = 1.0339
epoch 2 iter 52300: loss = 1.0868,  smooth loss = 1.0310
epoch 2 iter 52350: loss = 0.9784,  smooth loss = 1.0263
epoch 2 iter 52400: loss = 1.2090,  smooth loss = 1.0334
epoch 2 iter 52450: loss = 0.8751,  smooth loss = 1.0211
epoch 2 iter 52500: loss = 1.1178,  smooth loss = 1.0298
epoch 2 iter 52550: loss = 0.9952,  smooth loss = 1.0289
epoch 2 iter 52600: loss = 0.9475,  smooth loss = 1.0219
epoch 2 iter 52650: loss = 1.1501,  smooth loss = 1.0178
epoch 2 iter 52700: loss = 1.0916,  smooth loss = 1.0373
epoch 2 iter 52750: loss = 1.1345,  smooth loss = 1.0207
epoch 2 iter 52800: loss = 1.0033,  smooth loss = 1.0230
epoch 2 iter 52850: loss = 1.0335,  smooth loss = 1.0073
epoch 2 iter 52900: loss = 0.9162,  smooth loss = 1.0158
epoch 2 iter 52950: loss = 1.0199,  smooth loss = 1.0211
epoch 2 iter 53000: loss = 1.0823,  smooth loss = 1.0293
average data time = 0.0031s, average running time = 0.8589s
epoch 2 iter 53000: eval loss = 0.6092,  ccr = 0.9022,  cwr = 0.7993,  ted = 1529.0000,  ned = 606.8402,  ted/w = 0.3689, 
Save model conclr-pretrain-vision-model_2_53000
epoch 2 iter 53050: loss = 0.9854,  smooth loss = 1.0116
epoch 2 iter 53100: loss = 1.1354,  smooth loss = 1.0289
epoch 2 iter 53150: loss = 0.9559,  smooth loss = 1.0112
epoch 2 iter 53200: loss = 1.0912,  smooth loss = 1.0079
epoch 2 iter 53250: loss = 1.0996,  smooth loss = 1.0175
epoch 2 iter 53300: loss = 1.0803,  smooth loss = 1.0317
epoch 2 iter 53350: loss = 0.8923,  smooth loss = 1.0072
epoch 2 iter 53400: loss = 1.0477,  smooth loss = 1.0079
epoch 2 iter 53450: loss = 0.9542,  smooth loss = 1.0252
epoch 2 iter 53500: loss = 1.0134,  smooth loss = 1.0157
epoch 2 iter 53550: loss = 0.9160,  smooth loss = 1.0209
epoch 2 iter 53600: loss = 0.9273,  smooth loss = 1.0226
epoch 2 iter 53650: loss = 1.1715,  smooth loss = 1.0138
epoch 2 iter 53700: loss = 1.0089,  smooth loss = 1.0290
epoch 2 iter 53750: loss = 0.9452,  smooth loss = 1.0205
epoch 2 iter 53800: loss = 1.0552,  smooth loss = 1.0244
epoch 2 iter 53850: loss = 1.1358,  smooth loss = 1.0206
epoch 2 iter 53900: loss = 1.0114,  smooth loss = 1.0184
epoch 2 iter 53950: loss = 0.9589,  smooth loss = 1.0082
epoch 2 iter 54000: loss = 0.9102,  smooth loss = 1.0110
average data time = 0.0030s, average running time = 0.8589s
epoch 2 iter 54000: eval loss = 0.5092,  ccr = 0.9125,  cwr = 0.8256,  ted = 1290.0000,  ned = 500.3757,  ted/w = 0.3112, 
Save model conclr-pretrain-vision-model_2_54000
epoch 2 iter 54050: loss = 0.9826,  smooth loss = 1.0068
epoch 2 iter 54100: loss = 1.0693,  smooth loss = 0.9966
epoch 2 iter 54150: loss = 1.0540,  smooth loss = 1.0158
epoch 2 iter 54200: loss = 1.0709,  smooth loss = 1.0333
epoch 2 iter 54250: loss = 0.9923,  smooth loss = 1.0435
epoch 2 iter 54300: loss = 1.0525,  smooth loss = 1.0304
epoch 2 iter 54350: loss = 0.9647,  smooth loss = 1.0414
epoch 2 iter 54400: loss = 1.0084,  smooth loss = 1.0333
epoch 2 iter 54450: loss = 1.0571,  smooth loss = 1.0197
epoch 3 iter 54500: loss = 1.0365,  smooth loss = 1.0252
epoch 3 iter 54550: loss = 1.0014,  smooth loss = 1.0004
epoch 3 iter 54600: loss = 1.0271,  smooth loss = 0.9945
epoch 3 iter 54650: loss = 0.9000,  smooth loss = 0.9886
epoch 3 iter 54700: loss = 0.9245,  smooth loss = 0.9928
epoch 3 iter 54750: loss = 1.0633,  smooth loss = 0.9790
epoch 3 iter 54800: loss = 0.9753,  smooth loss = 0.9860
epoch 3 iter 54850: loss = 1.0194,  smooth loss = 0.9733
epoch 3 iter 54900: loss = 0.8591,  smooth loss = 0.9666
epoch 3 iter 54950: loss = 0.9168,  smooth loss = 0.9693
epoch 3 iter 55000: loss = 0.7612,  smooth loss = 0.9563
average data time = 0.0033s, average running time = 0.8589s
epoch 3 iter 55000: eval loss = 0.5694,  ccr = 0.9119,  cwr = 0.8039,  ted = 1419.0000,  ned = 616.5678,  ted/w = 0.3423, 
Save model conclr-pretrain-vision-model_3_55000
epoch 3 iter 55050: loss = 0.9342,  smooth loss = 0.9746
epoch 3 iter 55100: loss = 0.8697,  smooth loss = 0.9757
epoch 3 iter 55150: loss = 0.9594,  smooth loss = 0.9705
epoch 3 iter 55200: loss = 0.9060,  smooth loss = 0.9545
epoch 3 iter 55250: loss = 0.9104,  smooth loss = 0.9744
epoch 3 iter 55300: loss = 1.0510,  smooth loss = 0.9728
epoch 3 iter 55350: loss = 0.9592,  smooth loss = 0.9681
epoch 3 iter 55400: loss = 1.0827,  smooth loss = 0.9653
epoch 3 iter 55450: loss = 1.1983,  smooth loss = 0.9737
epoch 3 iter 55500: loss = 1.0511,  smooth loss = 0.9766
epoch 3 iter 55550: loss = 0.9068,  smooth loss = 0.9679
epoch 3 iter 55600: loss = 0.8849,  smooth loss = 0.9520
epoch 3 iter 55650: loss = 0.9923,  smooth loss = 0.9649
epoch 3 iter 55700: loss = 0.9307,  smooth loss = 0.9657
epoch 3 iter 55750: loss = 0.9018,  smooth loss = 0.9717
epoch 3 iter 55800: loss = 0.9527,  smooth loss = 0.9680
epoch 3 iter 55850: loss = 0.8438,  smooth loss = 0.9643
epoch 3 iter 55900: loss = 1.0663,  smooth loss = 0.9455
epoch 3 iter 55950: loss = 1.0052,  smooth loss = 0.9674
epoch 3 iter 56000: loss = 0.8463,  smooth loss = 0.9753
average data time = 0.0032s, average running time = 0.8589s
epoch 3 iter 56000: eval loss = 0.5488,  ccr = 0.9172,  cwr = 0.8347,  ted = 1309.0000,  ned = 559.3404,  ted/w = 0.3158, 
Better model found at epoch 3, iter 56000 with accuracy value: 0.8347.
Save model conclr-pretrain-vision-model_3_56000
epoch 3 iter 56050: loss = 0.9186,  smooth loss = 0.9640
epoch 3 iter 56100: loss = 0.9079,  smooth loss = 0.9594
epoch 3 iter 56150: loss = 0.9402,  smooth loss = 0.9599
epoch 3 iter 56200: loss = 0.8611,  smooth loss = 0.9722
epoch 3 iter 56250: loss = 1.1506,  smooth loss = 0.9662
epoch 3 iter 56300: loss = 0.8708,  smooth loss = 0.9582
epoch 3 iter 56350: loss = 0.8422,  smooth loss = 0.9632
epoch 3 iter 56400: loss = 0.9958,  smooth loss = 0.9685
epoch 3 iter 56450: loss = 1.0755,  smooth loss = 0.9630
epoch 3 iter 56500: loss = 0.9247,  smooth loss = 0.9571
epoch 3 iter 56550: loss = 0.8854,  smooth loss = 0.9628
epoch 3 iter 56600: loss = 1.0466,  smooth loss = 0.9662
epoch 3 iter 56650: loss = 0.8526,  smooth loss = 0.9538
epoch 3 iter 56700: loss = 0.8959,  smooth loss = 0.9531
epoch 3 iter 56750: loss = 0.8518,  smooth loss = 0.9500
epoch 3 iter 56800: loss = 0.9874,  smooth loss = 0.9542
epoch 3 iter 56850: loss = 0.8659,  smooth loss = 0.9545
epoch 3 iter 56900: loss = 0.9039,  smooth loss = 0.9635
epoch 3 iter 56950: loss = 0.9586,  smooth loss = 0.9609
epoch 3 iter 57000: loss = 1.0391,  smooth loss = 0.9546
average data time = 0.0032s, average running time = 0.8587s
epoch 3 iter 57000: eval loss = 0.5798,  ccr = 0.9169,  cwr = 0.8200,  ted = 1386.0000,  ned = 610.1051,  ted/w = 0.3344, 
Save model conclr-pretrain-vision-model_3_57000
epoch 3 iter 57050: loss = 0.8047,  smooth loss = 0.9458
epoch 3 iter 57100: loss = 0.9828,  smooth loss = 0.9472
epoch 3 iter 57150: loss = 1.0177,  smooth loss = 0.9581
epoch 3 iter 57200: loss = 1.1215,  smooth loss = 0.9633
epoch 3 iter 57250: loss = 0.8882,  smooth loss = 0.9553
epoch 3 iter 57300: loss = 0.9931,  smooth loss = 0.9496
epoch 3 iter 57350: loss = 0.9018,  smooth loss = 0.9565
epoch 3 iter 57400: loss = 0.8213,  smooth loss = 0.9515
epoch 3 iter 57450: loss = 0.9910,  smooth loss = 0.9544
epoch 3 iter 57500: loss = 0.8827,  smooth loss = 0.9564
epoch 3 iter 57550: loss = 1.0824,  smooth loss = 0.9676
epoch 3 iter 57600: loss = 0.8971,  smooth loss = 0.9578
epoch 3 iter 57650: loss = 0.9737,  smooth loss = 0.9523
epoch 3 iter 57700: loss = 1.0207,  smooth loss = 0.9350
epoch 3 iter 57750: loss = 1.0478,  smooth loss = 0.9637
epoch 3 iter 57800: loss = 0.9201,  smooth loss = 0.9436
epoch 3 iter 57850: loss = 0.9604,  smooth loss = 0.9593
epoch 3 iter 57900: loss = 0.9180,  smooth loss = 0.9649
epoch 3 iter 57950: loss = 1.0782,  smooth loss = 0.9527
epoch 3 iter 58000: loss = 0.8946,  smooth loss = 0.9596
average data time = 0.0032s, average running time = 0.8587s
epoch 3 iter 58000: eval loss = 0.6137,  ccr = 0.9097,  cwr = 0.8188,  ted = 1393.0000,  ned = 616.2623,  ted/w = 0.3361, 
Save model conclr-pretrain-vision-model_3_58000
epoch 3 iter 58050: loss = 0.9019,  smooth loss = 0.9497
epoch 3 iter 58100: loss = 0.9805,  smooth loss = 0.9472
epoch 3 iter 58150: loss = 0.9102,  smooth loss = 0.9447
epoch 3 iter 58200: loss = 0.9721,  smooth loss = 0.9559
epoch 3 iter 58250: loss = 0.9256,  smooth loss = 0.9484
epoch 3 iter 58300: loss = 1.1046,  smooth loss = 0.9542
epoch 3 iter 58350: loss = 0.8913,  smooth loss = 0.9420
epoch 3 iter 58400: loss = 0.8967,  smooth loss = 0.9488
epoch 3 iter 58450: loss = 0.8858,  smooth loss = 0.9481
epoch 3 iter 58500: loss = 0.9090,  smooth loss = 0.9281
epoch 3 iter 58550: loss = 1.0492,  smooth loss = 0.9285
epoch 3 iter 58600: loss = 0.8988,  smooth loss = 0.9533
epoch 3 iter 58650: loss = 1.0712,  smooth loss = 0.9503
epoch 3 iter 58700: loss = 0.7846,  smooth loss = 0.9409
epoch 3 iter 58750: loss = 0.8762,  smooth loss = 0.9518
epoch 3 iter 58800: loss = 0.8960,  smooth loss = 0.9558
epoch 3 iter 58850: loss = 0.9269,  smooth loss = 0.9543
epoch 3 iter 58900: loss = 0.8575,  smooth loss = 0.9443
epoch 3 iter 58950: loss = 1.0333,  smooth loss = 0.9469
epoch 3 iter 59000: loss = 0.9958,  smooth loss = 0.9447
average data time = 0.0032s, average running time = 0.8587s
epoch 3 iter 59000: eval loss = 0.6125,  ccr = 0.9136,  cwr = 0.8224,  ted = 1297.0000,  ned = 562.4591,  ted/w = 0.3129, 
Save model conclr-pretrain-vision-model_3_59000
epoch 3 iter 59050: loss = 0.9204,  smooth loss = 0.9380
epoch 3 iter 59100: loss = 0.9663,  smooth loss = 0.9378
epoch 3 iter 59150: loss = 0.9110,  smooth loss = 0.9570
epoch 3 iter 59200: loss = 0.9840,  smooth loss = 0.9468
epoch 3 iter 59250: loss = 1.1031,  smooth loss = 0.9502
epoch 3 iter 59300: loss = 0.9583,  smooth loss = 0.9539
epoch 3 iter 59350: loss = 1.0885,  smooth loss = 0.9526
epoch 3 iter 59400: loss = 0.9622,  smooth loss = 0.9495
epoch 3 iter 59450: loss = 0.9496,  smooth loss = 0.9406
epoch 3 iter 59500: loss = 0.9848,  smooth loss = 0.9477
epoch 3 iter 59550: loss = 0.9749,  smooth loss = 0.9531
epoch 3 iter 59600: loss = 1.0989,  smooth loss = 0.9432
epoch 3 iter 59650: loss = 0.9979,  smooth loss = 0.9448
epoch 3 iter 59700: loss = 0.9208,  smooth loss = 0.9551
epoch 3 iter 59750: loss = 1.0709,  smooth loss = 0.9533
epoch 3 iter 59800: loss = 1.0032,  smooth loss = 0.9547
epoch 3 iter 59850: loss = 0.8629,  smooth loss = 0.9513
epoch 3 iter 59900: loss = 0.9296,  smooth loss = 0.9584
epoch 3 iter 59950: loss = 0.8986,  smooth loss = 0.9586
epoch 3 iter 60000: loss = 0.8914,  smooth loss = 0.9595
average data time = 0.0031s, average running time = 0.8587s
epoch 3 iter 60000: eval loss = 0.5996,  ccr = 0.9149,  cwr = 0.8097,  ted = 1400.0000,  ned = 584.6686,  ted/w = 0.3378, 
Save model conclr-pretrain-vision-model_3_60000
epoch 3 iter 60050: loss = 0.8342,  smooth loss = 0.9635
epoch 3 iter 60100: loss = 1.0189,  smooth loss = 0.9554
epoch 3 iter 60150: loss = 1.0498,  smooth loss = 0.9481
epoch 3 iter 60200: loss = 0.7590,  smooth loss = 0.9435
epoch 3 iter 60250: loss = 0.9684,  smooth loss = 0.9471
epoch 3 iter 60300: loss = 0.7751,  smooth loss = 0.9353
epoch 3 iter 60350: loss = 1.0916,  smooth loss = 0.9496
epoch 3 iter 60400: loss = 0.9336,  smooth loss = 0.9563
epoch 3 iter 60450: loss = 0.9878,  smooth loss = 0.9558
epoch 3 iter 60500: loss = 1.0224,  smooth loss = 0.9465
epoch 3 iter 60550: loss = 1.0480,  smooth loss = 0.9473
epoch 3 iter 60600: loss = 0.9399,  smooth loss = 0.9347
epoch 3 iter 60650: loss = 1.0198,  smooth loss = 0.9369
epoch 3 iter 60700: loss = 0.9719,  smooth loss = 0.9414
epoch 3 iter 60750: loss = 0.8634,  smooth loss = 0.9436
epoch 3 iter 60800: loss = 1.0000,  smooth loss = 0.9462
epoch 3 iter 60850: loss = 0.9497,  smooth loss = 0.9415
epoch 3 iter 60900: loss = 0.8443,  smooth loss = 0.9390
epoch 3 iter 60950: loss = 0.9951,  smooth loss = 0.9412
epoch 3 iter 61000: loss = 0.9523,  smooth loss = 0.9515
average data time = 0.0031s, average running time = 0.8588s
epoch 3 iter 61000: eval loss = 0.6380,  ccr = 0.9139,  cwr = 0.8089,  ted = 1448.0000,  ned = 617.1582,  ted/w = 0.3493, 
Save model conclr-pretrain-vision-model_3_61000
epoch 3 iter 61050: loss = 0.9128,  smooth loss = 0.9451
epoch 3 iter 61100: loss = 1.0369,  smooth loss = 0.9458
epoch 3 iter 61150: loss = 0.8766,  smooth loss = 0.9401
epoch 3 iter 61200: loss = 0.8932,  smooth loss = 0.9474
epoch 3 iter 61250: loss = 0.8326,  smooth loss = 0.9466
epoch 3 iter 61300: loss = 0.9018,  smooth loss = 0.9345
epoch 3 iter 61350: loss = 1.0320,  smooth loss = 0.9419
epoch 3 iter 61400: loss = 0.9833,  smooth loss = 0.9406
epoch 3 iter 61450: loss = 0.9566,  smooth loss = 0.9503
epoch 3 iter 61500: loss = 0.8639,  smooth loss = 0.9474
epoch 3 iter 61550: loss = 0.9183,  smooth loss = 0.9517
epoch 3 iter 61600: loss = 1.0021,  smooth loss = 0.9504
epoch 3 iter 61650: loss = 1.0879,  smooth loss = 0.9522
epoch 3 iter 61700: loss = 0.8019,  smooth loss = 0.9418
epoch 3 iter 61750: loss = 1.1647,  smooth loss = 0.9476
epoch 3 iter 61800: loss = 0.9354,  smooth loss = 0.9434
epoch 3 iter 61850: loss = 0.9249,  smooth loss = 0.9466
epoch 3 iter 61900: loss = 0.9903,  smooth loss = 0.9472
epoch 3 iter 61950: loss = 0.9943,  smooth loss = 0.9489
epoch 3 iter 62000: loss = 0.8903,  smooth loss = 0.9398
average data time = 0.0031s, average running time = 0.8588s
epoch 3 iter 62000: eval loss = 0.6731,  ccr = 0.9065,  cwr = 0.8104,  ted = 1434.0000,  ned = 601.2839,  ted/w = 0.3460, 
Save model conclr-pretrain-vision-model_3_62000
epoch 3 iter 62050: loss = 0.9276,  smooth loss = 0.9443
epoch 3 iter 62100: loss = 0.9443,  smooth loss = 0.9486
epoch 3 iter 62150: loss = 1.0023,  smooth loss = 0.9442
epoch 3 iter 62200: loss = 0.8363,  smooth loss = 0.9371
epoch 3 iter 62250: loss = 0.9843,  smooth loss = 0.9555
epoch 3 iter 62300: loss = 1.0853,  smooth loss = 0.9609
epoch 3 iter 62350: loss = 0.8794,  smooth loss = 0.9472
epoch 3 iter 62400: loss = 0.9762,  smooth loss = 0.9446
epoch 3 iter 62450: loss = 1.0107,  smooth loss = 0.9359
epoch 3 iter 62500: loss = 0.8808,  smooth loss = 0.9303
epoch 3 iter 62550: loss = 0.9209,  smooth loss = 0.9456
epoch 3 iter 62600: loss = 0.8949,  smooth loss = 0.9524
epoch 3 iter 62650: loss = 1.0027,  smooth loss = 0.9348
epoch 3 iter 62700: loss = 0.8744,  smooth loss = 0.9465
epoch 3 iter 62750: loss = 0.8566,  smooth loss = 0.9434
epoch 3 iter 62800: loss = 0.8608,  smooth loss = 0.9320
epoch 3 iter 62850: loss = 0.9525,  smooth loss = 0.9398
epoch 3 iter 62900: loss = 0.9859,  smooth loss = 0.9346
epoch 3 iter 62950: loss = 0.9135,  smooth loss = 0.9351
epoch 3 iter 63000: loss = 0.9076,  smooth loss = 0.9530
average data time = 0.0031s, average running time = 0.8588s
epoch 3 iter 63000: eval loss = 0.6062,  ccr = 0.9089,  cwr = 0.8241,  ted = 1397.0000,  ned = 562.3270,  ted/w = 0.3370, 
Save model conclr-pretrain-vision-model_3_63000
epoch 3 iter 63050: loss = 0.8398,  smooth loss = 0.9467
epoch 3 iter 63100: loss = 0.8638,  smooth loss = 0.9317
epoch 3 iter 63150: loss = 0.8945,  smooth loss = 0.9391
epoch 3 iter 63200: loss = 0.9814,  smooth loss = 0.9492
epoch 3 iter 63250: loss = 0.8615,  smooth loss = 0.9452
epoch 3 iter 63300: loss = 0.7897,  smooth loss = 0.9480
epoch 3 iter 63350: loss = 0.8826,  smooth loss = 0.9430
epoch 3 iter 63400: loss = 1.0261,  smooth loss = 0.9430
epoch 3 iter 63450: loss = 0.9716,  smooth loss = 0.9600
epoch 3 iter 63500: loss = 1.0413,  smooth loss = 0.9652
epoch 3 iter 63550: loss = 1.0034,  smooth loss = 0.9582
epoch 3 iter 63600: loss = 0.9983,  smooth loss = 0.9457
epoch 3 iter 63650: loss = 1.0028,  smooth loss = 0.9392
epoch 3 iter 63700: loss = 0.8077,  smooth loss = 0.9415
epoch 3 iter 63750: loss = 1.0044,  smooth loss = 0.9442
epoch 3 iter 63800: loss = 0.9241,  smooth loss = 0.9394
epoch 3 iter 63850: loss = 0.9221,  smooth loss = 0.9382
epoch 3 iter 63900: loss = 0.9960,  smooth loss = 0.9372
epoch 3 iter 63950: loss = 0.9491,  smooth loss = 0.9267
epoch 3 iter 64000: loss = 0.8683,  smooth loss = 0.9341
average data time = 0.0030s, average running time = 0.8588s
epoch 3 iter 64000: eval loss = 0.6068,  ccr = 0.9097,  cwr = 0.8227,  ted = 1450.0000,  ned = 576.4284,  ted/w = 0.3498, 
Save model conclr-pretrain-vision-model_3_64000
epoch 3 iter 64050: loss = 0.9236,  smooth loss = 0.9295
epoch 3 iter 64100: loss = 0.7693,  smooth loss = 0.9309
epoch 3 iter 64150: loss = 0.9009,  smooth loss = 0.9329
epoch 3 iter 64200: loss = 0.9408,  smooth loss = 0.9311
epoch 3 iter 64250: loss = 0.9031,  smooth loss = 0.9398
epoch 3 iter 64300: loss = 0.9782,  smooth loss = 0.9390
epoch 3 iter 64350: loss = 0.9183,  smooth loss = 0.9349
epoch 3 iter 64400: loss = 1.0862,  smooth loss = 0.9528
epoch 3 iter 64450: loss = 0.9009,  smooth loss = 0.9281
epoch 3 iter 64500: loss = 0.8865,  smooth loss = 0.9416
epoch 3 iter 64550: loss = 0.9994,  smooth loss = 0.9404
epoch 3 iter 64600: loss = 0.9064,  smooth loss = 0.9337
epoch 3 iter 64650: loss = 0.9960,  smooth loss = 0.9408
epoch 3 iter 64700: loss = 0.9913,  smooth loss = 0.9365
epoch 3 iter 64750: loss = 0.9417,  smooth loss = 0.9484
epoch 3 iter 64800: loss = 1.0138,  smooth loss = 0.9449
epoch 3 iter 64850: loss = 1.0348,  smooth loss = 0.9404
epoch 3 iter 64900: loss = 0.9184,  smooth loss = 0.9372
epoch 3 iter 64950: loss = 0.9747,  smooth loss = 0.9431
epoch 3 iter 65000: loss = 1.0131,  smooth loss = 0.9404
average data time = 0.0030s, average running time = 0.8588s
epoch 3 iter 65000: eval loss = 0.6645,  ccr = 0.9119,  cwr = 0.8080,  ted = 1414.0000,  ned = 608.7121,  ted/w = 0.3411, 
Save model conclr-pretrain-vision-model_3_65000
epoch 3 iter 65050: loss = 0.9662,  smooth loss = 0.9298
epoch 3 iter 65100: loss = 1.0186,  smooth loss = 0.9519
epoch 3 iter 65150: loss = 0.9929,  smooth loss = 0.9446
epoch 3 iter 65200: loss = 0.8828,  smooth loss = 0.9513
epoch 3 iter 65250: loss = 0.8433,  smooth loss = 0.9408
epoch 3 iter 65300: loss = 0.9602,  smooth loss = 0.9329
epoch 3 iter 65350: loss = 0.9701,  smooth loss = 0.9325
epoch 3 iter 65400: loss = 0.8744,  smooth loss = 0.9279
epoch 3 iter 65450: loss = 0.8931,  smooth loss = 0.9308
epoch 3 iter 65500: loss = 0.9956,  smooth loss = 0.9557
epoch 3 iter 65550: loss = 0.8093,  smooth loss = 0.9398
epoch 3 iter 65600: loss = 0.9800,  smooth loss = 0.9284
epoch 3 iter 65650: loss = 0.8040,  smooth loss = 0.9476
epoch 3 iter 65700: loss = 1.0640,  smooth loss = 0.9503
epoch 3 iter 65750: loss = 1.1699,  smooth loss = 0.9596
epoch 3 iter 65800: loss = 0.9864,  smooth loss = 0.9359
epoch 3 iter 65850: loss = 0.9329,  smooth loss = 0.9347
epoch 3 iter 65900: loss = 1.0584,  smooth loss = 0.9308
epoch 3 iter 65950: loss = 1.0315,  smooth loss = 0.9372
epoch 3 iter 66000: loss = 0.8979,  smooth loss = 0.9276
average data time = 0.0030s, average running time = 0.8588s
epoch 3 iter 66000: eval loss = 0.5957,  ccr = 0.9158,  cwr = 0.8142,  ted = 1391.0000,  ned = 597.1645,  ted/w = 0.3356, 
Save model conclr-pretrain-vision-model_3_66000
epoch 3 iter 66050: loss = 0.7728,  smooth loss = 0.9292
epoch 3 iter 66100: loss = 1.0211,  smooth loss = 0.9316
epoch 3 iter 66150: loss = 0.9095,  smooth loss = 0.9343
epoch 3 iter 66200: loss = 1.0238,  smooth loss = 0.9286
epoch 3 iter 66250: loss = 1.0318,  smooth loss = 0.9406
epoch 3 iter 66300: loss = 0.9247,  smooth loss = 0.9471
epoch 3 iter 66350: loss = 1.0095,  smooth loss = 0.9355
epoch 3 iter 66400: loss = 0.8763,  smooth loss = 0.9413
epoch 3 iter 66450: loss = 0.8921,  smooth loss = 0.9504
epoch 3 iter 66500: loss = 0.9092,  smooth loss = 0.9423
epoch 3 iter 66550: loss = 1.0318,  smooth loss = 0.9372
epoch 3 iter 66600: loss = 0.8907,  smooth loss = 0.9324
epoch 3 iter 66650: loss = 0.8839,  smooth loss = 0.9315
epoch 3 iter 66700: loss = 0.9512,  smooth loss = 0.9294
epoch 3 iter 66750: loss = 0.8243,  smooth loss = 0.9305
epoch 3 iter 66800: loss = 0.9764,  smooth loss = 0.9442
epoch 3 iter 66850: loss = 0.8967,  smooth loss = 0.9545
epoch 3 iter 66900: loss = 0.9371,  smooth loss = 0.9478
epoch 3 iter 66950: loss = 0.9934,  smooth loss = 0.9500
epoch 3 iter 67000: loss = 0.9295,  smooth loss = 0.9449
average data time = 0.0030s, average running time = 0.8587s
epoch 3 iter 67000: eval loss = 0.5885,  ccr = 0.9213,  cwr = 0.8138,  ted = 1308.0000,  ned = 553.2986,  ted/w = 0.3156, 
Save model conclr-pretrain-vision-model_3_67000
epoch 3 iter 67050: loss = 0.9728,  smooth loss = 0.9423
epoch 3 iter 67100: loss = 0.9414,  smooth loss = 0.9374
epoch 3 iter 67150: loss = 0.8882,  smooth loss = 0.9331
epoch 3 iter 67200: loss = 1.0870,  smooth loss = 0.9322
epoch 3 iter 67250: loss = 1.0075,  smooth loss = 0.9512
epoch 3 iter 67300: loss = 0.8224,  smooth loss = 0.9425
epoch 3 iter 67350: loss = 0.8894,  smooth loss = 0.9388
epoch 3 iter 67400: loss = 1.0256,  smooth loss = 0.9384
epoch 3 iter 67450: loss = 1.0079,  smooth loss = 0.9443
epoch 3 iter 67500: loss = 0.9287,  smooth loss = 0.9497
epoch 3 iter 67550: loss = 1.0624,  smooth loss = 0.9455
epoch 3 iter 67600: loss = 1.0957,  smooth loss = 0.9483
epoch 3 iter 67650: loss = 0.9411,  smooth loss = 0.9420
epoch 3 iter 67700: loss = 0.8299,  smooth loss = 0.9313
epoch 3 iter 67750: loss = 0.8911,  smooth loss = 0.9353
epoch 3 iter 67800: loss = 1.0316,  smooth loss = 0.9398
epoch 3 iter 67850: loss = 1.0233,  smooth loss = 0.9460
epoch 3 iter 67900: loss = 0.9245,  smooth loss = 0.9347
epoch 3 iter 67950: loss = 0.8193,  smooth loss = 0.9240
epoch 3 iter 68000: loss = 0.9003,  smooth loss = 0.9295
average data time = 0.0030s, average running time = 0.8587s
epoch 3 iter 68000: eval loss = 0.5931,  ccr = 0.9195,  cwr = 0.8299,  ted = 1244.0000,  ned = 509.3154,  ted/w = 0.3001, 
Save model conclr-pretrain-vision-model_3_68000
epoch 3 iter 68050: loss = 0.8309,  smooth loss = 0.9312
epoch 3 iter 68100: loss = 1.1779,  smooth loss = 0.9462
epoch 3 iter 68150: loss = 0.9649,  smooth loss = 0.9456
epoch 3 iter 68200: loss = 0.9739,  smooth loss = 0.9377
epoch 3 iter 68250: loss = 0.9602,  smooth loss = 0.9504
epoch 3 iter 68300: loss = 0.9202,  smooth loss = 0.9350
epoch 3 iter 68350: loss = 0.8791,  smooth loss = 0.9363
epoch 3 iter 68400: loss = 0.8644,  smooth loss = 0.9347
epoch 3 iter 68450: loss = 1.0158,  smooth loss = 0.9331
epoch 3 iter 68500: loss = 0.8841,  smooth loss = 0.9446
epoch 3 iter 68550: loss = 1.0280,  smooth loss = 0.9418
epoch 3 iter 68600: loss = 0.8685,  smooth loss = 0.9404
epoch 3 iter 68650: loss = 0.8038,  smooth loss = 0.9340
epoch 3 iter 68700: loss = 1.0285,  smooth loss = 0.9291
epoch 3 iter 68750: loss = 0.9763,  smooth loss = 0.9440
epoch 3 iter 68800: loss = 1.0034,  smooth loss = 0.9269
epoch 3 iter 68850: loss = 0.8720,  smooth loss = 0.9249
epoch 3 iter 68900: loss = 0.9127,  smooth loss = 0.9264
epoch 3 iter 68950: loss = 0.9473,  smooth loss = 0.9325
epoch 3 iter 69000: loss = 0.9057,  smooth loss = 0.9378
average data time = 0.0029s, average running time = 0.8586s
epoch 3 iter 69000: eval loss = 0.6050,  ccr = 0.9131,  cwr = 0.8331,  ted = 1255.0000,  ned = 491.1108,  ted/w = 0.3028, 
Save model conclr-pretrain-vision-model_3_69000
epoch 3 iter 69050: loss = 0.9071,  smooth loss = 0.9288
epoch 3 iter 69100: loss = 0.9883,  smooth loss = 0.9343
epoch 3 iter 69150: loss = 0.8698,  smooth loss = 0.9301
epoch 3 iter 69200: loss = 0.9379,  smooth loss = 0.9418
epoch 3 iter 69250: loss = 0.9592,  smooth loss = 0.9428
epoch 3 iter 69300: loss = 0.8824,  smooth loss = 0.9409
epoch 3 iter 69350: loss = 0.9502,  smooth loss = 0.9321
epoch 3 iter 69400: loss = 0.9742,  smooth loss = 0.9458
epoch 3 iter 69450: loss = 1.0831,  smooth loss = 0.9438
epoch 3 iter 69500: loss = 0.9217,  smooth loss = 0.9334
epoch 3 iter 69550: loss = 0.7915,  smooth loss = 0.9298
epoch 3 iter 69600: loss = 0.8067,  smooth loss = 0.9332
epoch 3 iter 69650: loss = 0.8756,  smooth loss = 0.9268
epoch 3 iter 69700: loss = 0.9076,  smooth loss = 0.9216
epoch 3 iter 69750: loss = 0.8038,  smooth loss = 0.9274
epoch 3 iter 69800: loss = 0.8692,  smooth loss = 0.9196
epoch 3 iter 69850: loss = 0.9515,  smooth loss = 0.9359
epoch 3 iter 69900: loss = 0.9714,  smooth loss = 0.9348
epoch 3 iter 69950: loss = 0.8039,  smooth loss = 0.9313
epoch 3 iter 70000: loss = 0.8249,  smooth loss = 0.9261
average data time = 0.0029s, average running time = 0.8586s
epoch 3 iter 70000: eval loss = 0.5569,  ccr = 0.9177,  cwr = 0.8195,  ted = 1245.0000,  ned = 539.1537,  ted/w = 0.3004, 
Save model conclr-pretrain-vision-model_3_70000
epoch 3 iter 70050: loss = 0.9334,  smooth loss = 0.9225
epoch 3 iter 70100: loss = 0.8739,  smooth loss = 0.9282
epoch 3 iter 70150: loss = 0.8743,  smooth loss = 0.9450
epoch 3 iter 70200: loss = 0.8282,  smooth loss = 0.9361
epoch 3 iter 70250: loss = 1.1161,  smooth loss = 0.9421
epoch 3 iter 70300: loss = 1.0316,  smooth loss = 0.9398
epoch 3 iter 70350: loss = 0.9214,  smooth loss = 0.9286
epoch 3 iter 70400: loss = 1.0802,  smooth loss = 0.9497
epoch 3 iter 70450: loss = 0.8565,  smooth loss = 0.9371
epoch 3 iter 70500: loss = 0.8926,  smooth loss = 0.9450
epoch 3 iter 70550: loss = 0.9249,  smooth loss = 0.9490
epoch 3 iter 70600: loss = 0.9604,  smooth loss = 0.9295
epoch 3 iter 70650: loss = 0.9432,  smooth loss = 0.9325
epoch 3 iter 70700: loss = 0.9114,  smooth loss = 0.9209
epoch 3 iter 70750: loss = 0.9738,  smooth loss = 0.9291
epoch 3 iter 70800: loss = 0.8333,  smooth loss = 0.9231
epoch 3 iter 70850: loss = 0.9011,  smooth loss = 0.9299
epoch 3 iter 70900: loss = 0.9386,  smooth loss = 0.9519
epoch 3 iter 70950: loss = 0.8590,  smooth loss = 0.9314
epoch 3 iter 71000: loss = 1.0598,  smooth loss = 0.9353
average data time = 0.0029s, average running time = 0.8585s
epoch 3 iter 71000: eval loss = 0.5955,  ccr = 0.9174,  cwr = 0.8101,  ted = 1350.0000,  ned = 587.9787,  ted/w = 0.3257, 
Save model conclr-pretrain-vision-model_3_71000
epoch 3 iter 71050: loss = 0.9924,  smooth loss = 0.9567
epoch 3 iter 71100: loss = 1.0199,  smooth loss = 0.9386
epoch 3 iter 71150: loss = 1.0032,  smooth loss = 0.9377
epoch 3 iter 71200: loss = 0.9591,  smooth loss = 0.9326
epoch 3 iter 71250: loss = 1.0347,  smooth loss = 0.9281
epoch 3 iter 71300: loss = 0.7693,  smooth loss = 0.9389
epoch 3 iter 71350: loss = 0.9443,  smooth loss = 0.9491
epoch 3 iter 71400: loss = 0.8697,  smooth loss = 0.9392
epoch 3 iter 71450: loss = 0.9186,  smooth loss = 0.9512
epoch 3 iter 71500: loss = 0.8850,  smooth loss = 0.9398
epoch 3 iter 71550: loss = 0.9503,  smooth loss = 0.9498
epoch 3 iter 71600: loss = 0.9228,  smooth loss = 0.9322
epoch 3 iter 71650: loss = 1.1433,  smooth loss = 0.9461
epoch 3 iter 71700: loss = 0.8208,  smooth loss = 0.9336
epoch 3 iter 71750: loss = 0.9126,  smooth loss = 0.9314
epoch 3 iter 71800: loss = 0.9185,  smooth loss = 0.9294
epoch 3 iter 71850: loss = 0.9850,  smooth loss = 0.9320
epoch 3 iter 71900: loss = 0.8826,  smooth loss = 0.9273
epoch 3 iter 71950: loss = 0.8363,  smooth loss = 0.9135
epoch 3 iter 72000: loss = 0.8999,  smooth loss = 0.9370
average data time = 0.0029s, average running time = 0.8585s
epoch 3 iter 72000: eval loss = 0.5675,  ccr = 0.9186,  cwr = 0.8097,  ted = 1359.0000,  ned = 606.1278,  ted/w = 0.3279, 
Save model conclr-pretrain-vision-model_3_72000
epoch 3 iter 72050: loss = 0.8567,  smooth loss = 0.9399
epoch 3 iter 72100: loss = 0.8786,  smooth loss = 0.9235
epoch 3 iter 72150: loss = 0.9751,  smooth loss = 0.9379
epoch 3 iter 72200: loss = 0.7278,  smooth loss = 0.9380
epoch 3 iter 72250: loss = 0.9665,  smooth loss = 0.9429
epoch 3 iter 72300: loss = 0.9442,  smooth loss = 0.9464
epoch 3 iter 72350: loss = 0.8579,  smooth loss = 0.9331
epoch 3 iter 72400: loss = 0.9603,  smooth loss = 0.9340
epoch 3 iter 72450: loss = 1.0513,  smooth loss = 0.9503
epoch 3 iter 72500: loss = 0.9931,  smooth loss = 0.9406
epoch 3 iter 72550: loss = 0.9909,  smooth loss = 0.9453
epoch 3 iter 72600: loss = 1.0066,  smooth loss = 0.9334
epoch 3 iter 72650: loss = 1.0305,  smooth loss = 0.9360
