* Context-Based Contrastive Learning for Scene Text Recognition

A code repository that attempts to implementation the [[https://ojs.aaai.org/index.php/AAAI/article/view/20245][ConCLR]] (AAAI 2022) (unofficial)

** Runtime Environment
install the dependencies
#+begin_src shell
pip install -r requirements.txt
#+end_src

** Training
1. Pretrain ABINet-Vision-ConCLR model
#+begin_src shell
CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py --config=configs/conclr_pretrain_vision_model.yaml
#+end_src

** Evaluation
1. Evaluate ABINet-Vision-ConCLR model
#+begin_src shell
CUDA_VISIBLE_DEVICES=0 python main.py --config=configs/conclr_pretrain_vision_model.yaml --phase test --image_only
#+end_src

** Run Demo
#+BEGIN_SRC shell
python demo.py --config=configs/conclr_pretrain_vision_model.yaml --input=test/img/path

Additional flags:
--config /path/to/config :: Set the path of configuration file
--input /path/to/image-directory :: Set the path of image directory or wildcard path
--checkpoint /path/to/checkpoint :: Set the path of trained model
--cuda [-1|0|1|2|3...] :: Set the cuda id, by default -1 is set and stands for cpu
--model_eval [alignment|vision] :: Specify which sub-model to use
--image_only :: Disable dumping visualization of attention masks
#+END_SRC

** Detail
The authors' key insight is that by pulling together embeddings of the same
character in different contexts and pushing apart embeddings of different
characters, we can guide models to learn a representation better balances the
intrinsic and context information. Here are some key components of the ConCLR.
*** Context-based Data Augmentation
Before feeding the batch data into the model, Context-based Data Augmentation
(ConAug) randomly permutes the input batch twice and randomly concatenates it to
the left or right of the original batch. As shown in figure below:
#+BEGIN_HTML
<div style="text-align: center;">
  <img src="./fig/conaug.png" alt="conaug.png" style="width: 430px;">
</div>
#+END_HTML
Because ConAug need to process the input data after organizing it into batch
data and before feeding it into the model, I use the Callback mechanism of the
fastai to implement it. I implemented the "on_batch_begin" method which will be
called before the batch feeding into model. Here is my implement code:

#+begin_src python
class ConAugPretransform(Callback):
    def __init__(self, *args, **kwargs):
        self.aug_type = kwargs["aug_type"]
        self.max_length = kwargs["max_length"]
        self.test_conaug = kwargs["test_conaug"]
        self.charset_path = kwargs["charset_path"]

    def _augment_images(self, images, permuted_indices, left):
        permuted_images = images[permuted_indices]
        augmented_images = torch.cat(
            [permuted_images if left else images, images if left else permuted_images],
            dim=3,
        )
        original_height, original_width = images.size(2), images.size(3)
        augmented_images = F.interpolate(
            augmented_images,
            size=(original_height, original_width),
            mode="bilinear",
            align_corners=False,
        )
        return augmented_images

    def _visualize_augmented_batch(self, x, x1, x2, y, y1, y2, yl, yl1, yl2):
        ...

    def _process_gt_labels(self, gt_labels, gt_lengths, permuted_indices, left):
        new_gt_labels = gt_labels.clone()
        new_gt_lengths = gt_lengths.clone()
        for i, perm in enumerate(permuted_indices):
            if left:
                new_gt = torch.cat(
                    (
                        gt_labels[perm][: gt_lengths[perm] - 1],
                        gt_labels[i][: self.max_length + 2 - gt_lengths[perm] - 1],
                        torch.unsqueeze(gt_labels[perm][-1], dim=0),
                    ),
                    dim=0,
                )
            else:
                new_gt = torch.cat(
                    (
                        gt_labels[i][: gt_lengths[i] - 1],
                        gt_labels[perm][: self.max_length + 2 - gt_lengths[i] - 1],
                        torch.unsqueeze(gt_labels[i][-1], dim=0),
                    ),
                    dim=0,
                )
            new_gt_labels[i] = new_gt
        new_gt_lengths += new_gt_lengths[permuted_indices] - 1
        new_gt_lengths = torch.minimum(
            new_gt_lengths, torch.tensor(self.max_length + 1)
        )
        return new_gt_labels, new_gt_lengths

    def on_batch_begin(self, last_input, last_target, **kwargs) -> dict:
        images = last_input[0]
        permuted_indices_one = torch.randperm(images.size(0))
        permuted_indices_two = torch.randperm(images.size(0))

        left_one = torch.randint(0, 2, (1,), dtype=torch.bool)
        left_two = torch.randint(0, 2, (1,), dtype=torch.bool)

        augmented_batch_one = self._augment_images(
            images, permuted_indices_one, left_one
        )
        augmented_batch_two = self._augment_images(
            images, permuted_indices_two, left_two
        )

        last_input = ((images, augmented_batch_one, augmented_batch_two), last_input[1])

        gt_labels, gt_lengths = last_target[0], last_target[1]
        gt_labels_one, gt_lengths_one = self._process_gt_labels(
            gt_labels, gt_lengths, permuted_indices_one, left_one
        )
        gt_labels_two, gt_lengths_two = self._process_gt_labels(
            gt_labels, gt_lengths, permuted_indices_two, left_two
        )

        last_target[0] = [gt_labels, gt_labels_one, gt_labels_two]
        last_target[1] = [gt_lengths, gt_lengths_one, gt_lengths_two]

        if self.test_conaug:
            self._visualize_augmented_batch(
                images,
                augmented_batch_one,
                augmented_batch_two,
                gt_labels,
                gt_labels_one,
                gt_labels_two,
                gt_lengths,
                gt_lengths_one,
                gt_lengths_two,
            )

        return {"last_input": last_input, "last_target": last_target}
#+end_src

In this function, I process the augmented batch and its label together. (the
augmented batch is also used to calculate recognition loss so process the label
is necessary)

I plotted the results of conaug to verify the correctness of my implementation, as follows:
#+BEGIN_HTML
<div style="text-align: center;">
  <img src="./fig/aug1.png" alt="conaug.png" style="width: 1000px;">
</div>
#+END_HTML
#+BEGIN_HTML
<div style="text-align: center;">
  <img src="./fig/aug2.png" alt="conaug.png" style="width: 1000px;">
</div>
#+END_HTML
#+BEGIN_HTML
<div style="text-align: center;">
  <img src="./fig/aug3.png" alt="conaug.png" style="width: 1000px;">
</div>
#+END_HTML

*** Projection Head
Feed the two augmented batch into the backbone and decoder, we obtained their
corresponding glimpse vectors. The author mentions that [[https://arxiv.org/abs/2002.05709][SimCLR]] proposes to
utilize embeddings directly for contrastive learning will harm to the model
performance，thereby necessitating a projection head to filter out irrelevant
information. Additionally, I noted that SimCLR discusses the benefits of
incorporating non-linear layers in the projection head to enhance performance.
Consequently, I implemented the head using the following code.
#+begin_src python
class ProjectionHead(nn.Module):
    def __init__(self, in_channel, out_channel, head="mlp") -> None:
        super().__init__()
        if head == "linear":
            self.head = nn.Linear(in_channel, out_channel)
        if head == "mlp":
            self.head = nn.Sequential(
                nn.Linear(in_channel, in_channel),
                nn.ReLU(inplace=True),
                nn.Linear(in_channel, out_channel),
            )

    def forward(self, x):
        embed = F.normalize(self.head(x), dim=1)
        return embed
#+end_src

*** Contrastive Loss
By ConAug、Backbone、Decoder and the Projection Head, we have obtained two embeddings
corresponding the two augmented batches, and now how could we do the "pulling
together embeddings of the same character in different contexts and pushing
apart embeddings of different characters"? The anwser is the Contrastive Loss.
#+BEGIN_HTML
<div style="text-align: center;">
  <img src="./fig/contrastiveloss.png" alt="contrastiveloss.png" style="width: 800px;">
</div>
#+END_HTML
Here are the formulate:

$$
\mathcal{L} _ {pair}(\boldsymbol{T})=\sum _ {m \in M} \frac{-1}{|P(m)|}\sum _ {p \in P(m)} \left(\log \exp \left(\boldsymbol{z} _ m \cdot \boldsymbol{z} _ p / \tau\right) -\log\sum _ {a \in A(m)} \exp \left(\boldsymbol{z} _ m \cdot \boldsymbol{z} _ a / \tau\right)\right)
$$

$$
\mathcal{L} _ {clr}=\frac{1}{N}\sum _ {T \in I^{aug}}\mathcal{L} _ {pair}(\boldsymbol{T})
$$

I won’t go into details about the meaning of the letters in the specific formula
here, it can be found in the paper. My code is implemented as follows.

#+begin_src python
class ContrastiveLoss(nn.Module):
    def __init__(self, temprature=2):
        super().__init__()
        self.temprature = temprature

    def _clr_loss(self, logits1, logits2, labels1, labels2, loss_name, record=True):
        logits = torch.cat((logits1, logits2), dim=1)
        labels = torch.cat((labels1, labels2), dim=1)

        b, max_length = labels.shape
        am = (
            ~torch.eye(max_length, dtype=torch.bool)
            .unsqueeze(0)
            .expand(b, -1, -1)
            .cuda()
        )  # no self mask, set self false
        pm = (
            labels[:, :, None] != labels[:, None, :]
        ) & am  # positive mask, set positive false

        p_num = pm.sum(dim=1).unsqueeze(2)  # count pos num of each m
        npos = (
            am & p_num.expand(b, max_length, max_length).bool()
        )  # no positive label mask, set no pos false

        s = torch.div(
            logits @ logits.transpose(2, 1), self.temprature
        )  # calculate similarity
        s = torch.masked_fill(s, ~npos, 0)  # mask no pos
        s = torch.exp(s - s.max(dim=2, keepdim=True)[0])  # prevent overflow

        p = torch.masked_fill(s, ~pm, 0)
        a = torch.masked_fill(s, ~am, 0)
        a = torch.masked_fill(a, ~npos, 0).sum(dim=2)
        a = torch.where(a != 0, torch.log(a), 0)

        smx = torch.where(p != 0, p - a.unsqueeze(2), 0)
        l_pair = torch.sum(-torch.sum(smx, dim=2) / p_num.squeeze(), dim=1)
        loss = torch.mean(l_pair, dim=0)

        if record and loss_name is not None:
            self.losses[f"{loss_name}_loss"] = loss

        return loss

    def forward(self, X, Y, *args):
        self.losses = {}
        features, loss_name = X[0].get("proj"), X[0].get("name")
        return self._clr_loss(
            features[0],
            features[1],
            torch.argmax(Y[0], dim=2),
            torch.argmax(Y[1], dim=2),
            loss_name,
            *args,
        )
#+end_src

To improve parallel computation efficiency, I compute the contrastive loss
directly across the entire batch. First, I concatenate two embeddings along the
second dimension to obtain $z$. Then, by performing z @ z^{T} / \tau, I matrix-ify
the step of computing dot products in the formula. This yields a matrix $s$ of
shape (batchsize, max_length, max_length), where each element s_{ij}
represents z_{i}⋅z_{j}​. By applying masks and performing a logsumexp
operation, we obtain an equivalent result to the loss function. In practical
scenarios, some rows in matrix $s$ may be fully masked, resulting in logsumexp
values of inf. During the final loss computation, these are masked with a very
small value.

** Result

For model evaluation to demonstrate effectiveness of reproduction, I trained the
ConCLR-Vision and ABInet-Vision models using same training settings. Due to
constraints on time and computational resources, both models were trained on the
ST dataset for 4 epochs using four NVIDIA 2080 Ti GPUs (on AutoDL 🥲). The learning
rate was initialized at 1e-4 and decayed to 1e-6 by the final epoch.

#+BEGIN_HTML
<table>
  <tr>
    <th>Model</th>
    <th>Dataset</th>
    <th>CCR</th>
    <th>CWR</th>
  </tr>
  <tr>
    <td rowspan="3" style="text-align: center;">conclr-vision</td>
    <td>IIIT5k</td>
    <td>0.943</td>
    <td>0.836</td>
  </tr>
  <tr>
    <td>IC13</td>
    <td>0.948</td>
    <td>0.834</td>
  </tr>
  <tr>
    <td>CUTE80</td>
    <td>0.814</td>
    <td>0.875</td>
  </tr>
  <tr>
    <td rowspan="3" style="text-align: center;">abinet-vision</td>
    <td>IIIT5k</td>
    <td>0.926</td>
    <td>0.872</td>
  </tr>
  <tr>
    <td>IC13</td>
    <td>0.938</td>
    <td>0.852</td>
  </tr>
  <tr>
    <td>CUTE80</td>
    <td>0.771</td>
    <td>0.882</td>
  </tr>
</table>
#+END_HTML

From the results, using ConCLR has shown an improvement in CCR compared to the
baseline. I attribute this to the effectiveness of contrastive loss and ConAug.
However, the performance of CWR metrics was not satisfactory, possibly due to
the small size of the training dataset. Typically, methods involving contrastive
learning require larger datasets and more training time for optimization.
(Alternatively, there might be errors in my implementation, and I would greatly
appreciate any feedback on this matter 🥰)

More comprehensive experiments are underway...

** Conference
[[https://github.com/FangShancheng/ABINet][FangShancheng/ABINet]]
